{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjtZNlRzAytT"
      },
      "source": [
        "# Latam Data Engineer Challenge\n",
        "---\n",
        "## Configuración del Proyecto\n",
        "\n",
        "### Entorno en Google Cloud Platform (GCP)\n",
        "\n",
        "Para solventar el desafío, se configuró un entorno en Google Cloud Platform (GCP) con el nombre `development-latam-de`. Este entorno proporciona la infraestructura necesaria para manejar y procesar los datos del proyecto.\n",
        "\n",
        "### Arquitectura de Medallón\n",
        "\n",
        "En el contexto de la solución, se implementó una arquitectura de medallón, que organiza los datos en tres niveles distintos:\n",
        "\n",
        "1. **Bronze (Bronce)**: Datos en su forma cruda y original, que se identificará por el inicio `raw`.\n",
        "2. **Silver (Plata)**: Datos refinados y limpios, que se identificará por el inicio `cur`.\n",
        "3. **Gold (Oro)**: Datos altamente curados y optimizados para análisis avanzados, que se identificará por el inicio `mst`.\n",
        "\n",
        "Para este caso particular, se optó por crear tres carpetas dentro de un solo bucket en lugar de tres buckets separados. Esta estructura es ventajosa porque:\n",
        "\n",
        "- **Facilita la organización y gestión de datos**: Cada nivel tiene sus propios datos claramente segregados.\n",
        "- **Mejora la escalabilidad y mantenibilidad**: Es más fácil escalar y mantener el pipeline de datos cuando las etapas están bien definidas.\n",
        "- **Optimiza las políticas de seguridad y acceso**: Permite aplicar diferentes políticas de acceso y seguridad a cada nivel de datos.\n",
        "- **Aumenta el rendimiento de las consultas**: Al trabajar con datos más refinados en cada etapa, las consultas analíticas se vuelven más eficientes.\n",
        "\n",
        "## Pipeline de Procesamiento de Datos\n",
        "\n",
        "Se definió un pipeline que automatiza el proceso de creación de archivos curados y master. Este pipeline toma los datos crudos en formato `json` desde el bucket que se considera la data `raw`y los procesa adecuadamente.\n",
        "\n",
        "### Ejecución del Pipeline\n",
        "\n",
        "El pipeline se ejecuta manualmente utilizando GitHub Actions, lo cual ofrece varias ventajas:\n",
        "\n",
        "- **Automatización**: Garantiza que los procesos de ETL (Extracción, Transformación y Carga) sean automáticos y repetibles.\n",
        "- **Facilidad de uso**: GitHub Actions permite configurar y ejecutar workflows de manera sencilla.\n",
        "\n",
        "## Pruebas\n",
        "\n",
        "### Desarrollo Guiado por Pruebas (TDD)\n",
        "\n",
        "Se creó una carpeta llamada `tests` que contiene todos los archivos de prueba necesarios para el proyecto. Estas pruebas se diseñaron siguiendo la metodología de Desarrollo Guiado por Pruebas (TDD), lo que garantiza que:\n",
        "\n",
        "- **Funcionalidades validadas**: Cada funcionalidad del pipeline es probada y validada antes de su implementación.\n",
        "- **Código robusto**: Se minimizan los errores y se asegura la calidad del código.\n",
        "\n",
        "## Solución de Necesidades del Desafío\n",
        "\n",
        "Se abordaron y solucionaron las seis necesidades planteadas para el proyecto. Cada necesidad se resolvió de manera sistemática, asegurando que el pipeline de datos y las transformaciones cumplieran con los requisitos especificados.\n",
        "\n",
        "## Ejecución de Agregaciones y Publicación\n",
        "\n",
        "### Pipeline de Agregaciones\n",
        "\n",
        "Se definió un pipeline adicional para ejecutar las seis agregaciones necesarias. Este pipeline también se ejecuta mediante GitHub Actions, lo que asegura:\n",
        "\n",
        "- **Consistencia**: Las agregaciones se realizan de manera consistente cada vez que se ejecuta el pipeline.\n",
        "- **Repetibilidad**: El proceso es repetible, lo que facilita su ejecución en diferentes momentos sin cambios en los resultados.\n",
        "\n",
        "### Publicación del Notebook\n",
        "\n",
        "La documentación de este notebook se publica para proporcionar una referencia completa sobre la implementación y ejecución del proyecto. Esto incluye:\n",
        "\n",
        "- **Descripción del entorno**: Detalles sobre la configuración de GCP y la arquitectura de medallón.\n",
        "- **Detalles del pipeline**: Información sobre la creación y ejecución del pipeline de datos.\n",
        "- **Metodología de pruebas**: Enfoque TDD y estructura de las pruebas.\n",
        "- **Resolución de necesidades**: Explicación de cómo se resolvieron las necesidades específicas del proyecto.\n",
        "- **Pipeline de agregaciones**: Detalles sobre la ejecución y publicación de las agregaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrZDBPJrAytU"
      },
      "source": [
        "## Configuración del Entorno Colab\n",
        "\n",
        "Para configurar el entorno de Google Colab para el proyecto `challenge_de_latam`, se ejecutan varios pasos que son cruciales para preparar el entorno de desarrollo. A continuación, se explican cada uno de estos pasos y su propósito.\n",
        "\n",
        "### Montaje de Google Drive\n",
        "Primero, se monta Google Drive en el entorno de Colab. Esto permite acceder a los archivos almacenados en Google Drive directamente desde el entorno Colab, facilitando la carga y manipulación de datos, así como el acceso a scripts y recursos necesarios para el proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Navegación al Directorio del Proyecto\n",
        "\n",
        "Se cambia el directorio de trabajo al repositorio del proyecto en Google Drive. Esto asegura que todos los comandos y scripts que se ejecuten a continuación se realicen en el contexto correcto del proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/repos/challenge_de_latam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalación de Apache Spark y Dependencias del Proyecto\n",
        "\n",
        "Se ejecuta un script bash para instalar Apache Spark. Este paso garantiza que Spark esté disponible en el entorno de Colab.\n",
        "Seguido de ello, se instalan todas las dependencias necesarias especificadas en el archivo requirements.txt. Este archivo contiene todas las librerías y paquetes de Python que el proyecto necesita para ejecutarse correctamente.\n",
        "\n",
        "### Reinicio del Entorno de Colab\n",
        "\n",
        "Finalmente, se reinicia el entorno de Colab matando el proceso actual. Este paso es importante porque algunos cambios, como la instalación de Spark y otras dependencias, requieren un reinicio del kernel para aplicarse correctamente. Reiniciar el entorno garantiza que todas las instalaciones y configuraciones se apliquen de manera efectiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7_dompBIJx",
        "outputId": "ca7630cb-0f08-4b11-d96f-9aee2ee2b099"
      },
      "outputs": [],
      "source": [
        "!bash ./install_spark.sh\n",
        "!pip install -r requirements.txt\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ocq4W5TE3t3"
      },
      "source": [
        "# Importación de librerías\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmJkyKuDE90u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import findspark\n",
        "from google.cloud import storage, bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import userdata\n",
        "\n",
        "# Inicializar findspark antes de cualquier importación relacionada con Spark\n",
        "findspark.init()\n",
        "\n",
        "# Añadir el directorio del proyecto a la ruta del sistema\n",
        "sys.path.append('/content/drive/MyDrive/repos/challenge_de_latam/src')\n",
        "\n",
        "# Importar los módulos específicos del proyecto\n",
        "from q1_time import q1_time\n",
        "from q1_memory import q1_memory\n",
        "from q2_time import q2_time\n",
        "from q2_memory import q2_memory\n",
        "from q3_time import q3_time\n",
        "from q3_memory import q3_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUE4WxDKEqy7"
      },
      "source": [
        "# Obtención de los secretos almacenados en Google Colab\n",
        "---\n",
        "Se obtienen los secretos almacenados en Google Colab utilizando el módulo `userdata`. Este proceso es crucial para acceder de manera segura a las credenciales y otros secretos necesarios para interactuar con los servicios de Google Cloud Platform (GCP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVFv67-HFjcd"
      },
      "outputs": [],
      "source": [
        "gcp_service_account = userdata.get('gcp_service_account')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar las Credenciales desde la Cadena JSON\n",
        "---\n",
        "En esta sección, se cargan las credenciales de GCP desde una cadena JSON y se crean clientes autenticados para Google Cloud Storage y BigQuery. Este proceso es fundamental para interactuar de manera segura y autorizada con los servicios de GCP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEDYj70jFhki"
      },
      "outputs": [],
      "source": [
        "# Cargar las credenciales desde la cadena JSON\n",
        "credentials_info = json.loads(gcp_service_account)\n",
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "    credentials_info)\n",
        "# Crear el cliente de almacenamiento con las credenciales cargadas\n",
        "storage_client = storage.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])\n",
        "# Crear el cliente de BigQuery con las credenciales cargadas\n",
        "bigquery_client = bigquery.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operaciones con el Bucket de Google Cloud Storage\n",
        "---\n",
        "\n",
        "Se realizan varias operaciones con un bucket específico de Google Cloud Storage, incluyendo la descarga de un archivo Parquet a una ubicación temporal en el entorno de ejecución.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb2R3W3KEg0e",
        "outputId": "7d6bf556-d420-4a35-f324-07036475d40d"
      },
      "outputs": [],
      "source": [
        "\n",
        "BUCKET_NAME = 'dl-latam-dev'\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "# Operaciones con el bucket\n",
        "print(f\"Bucket {BUCKET_NAME} está listo para usar.\")\n",
        "\n",
        "# Definir el archivo Parquet y la ruta temporal\n",
        "FILE_NAME = 'mst_farmers_tweets.parquet'\n",
        "TEMP_FILE_PATH = '/tmp/mst_farmers_tweets.parquet'\n",
        "TABLE_NAME = \"mst_tweets.mst_farmers_tweets\"\n",
        "# Descargar el archivo Parquet a un archivo temporal\n",
        "blob = bucket.blob(FILE_NAME)\n",
        "blob.download_to_filename(TEMP_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN49ea4AytU"
      },
      "source": [
        "## Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de Tiempo en Archivos Parquet: Función `q1_time`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Pandas**:\n",
        "    - **Facilidad de Uso**: Pandas proporciona una interfaz sencilla y poderosa para la manipulación de datos, lo que facilita la carga y transformación de grandes datasets.\n",
        "    - **Eficiencia**: La función `pd.read_parquet` es eficiente en la carga de archivos Parquet, que están optimizados para consultas rápidas y almacenamiento compacto.\n",
        "\n",
        "2. **Optimización Temporal**:\n",
        "    - **Conversión de Fechas**: La conversión de la columna de fechas a objetos datetime y la extracción de solo la parte de la fecha permite un análisis más rápido y preciso de las fechas de los tweets.\n",
        "    - **Contado de Frecuencia**: El uso de `value_counts` en Pandas es altamente optimizado para contar frecuencias, permitiendo identificar rápidamente las fechas con más actividad.\n",
        "\n",
        "3. **Análisis de Usuarios**:\n",
        "    - **Identificación del Usuario Top**: Para cada una de las fechas más activas, se utiliza `value_counts` nuevamente para identificar el usuario con más tweets. Este enfoque es eficiente y directo.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, hay varias estrategias que se pueden implementar para mantener la eficiencia y escalabilidad del análisis:\n",
        "\n",
        "1. **Procesamiento en Chunks**:\n",
        "    - **Lectura Parcial de Datos**: Pandas permite la lectura de archivos en chunks, lo que reduce el uso de memoria y permite procesar datasets más grandes que la memoria disponible.\n",
        "\n",
        "    ```python\n",
        "    for chunk in pd.read_parquet(file_path, chunksize=100000):\n",
        "        # Procesar cada chunk individualmente\n",
        "    ```\n",
        "\n",
        "2. **Uso de Dask**:\n",
        "    - **Distribución de Carga**: Dask es una biblioteca que extiende Pandas para trabajar con datos distribuidos y en paralelo. Usar Dask en lugar de Pandas puede mejorar significativamente el rendimiento con grandes volúmenes de datos.\n",
        "\n",
        "    ```python\n",
        "    import dask.dataframe as dd\n",
        "    df = dd.read_parquet(file_path)\n",
        "    ```\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Indexación y Filtrado**: Asegurar que las columnas utilizadas para filtrar y agrupar (como la columna de fechas) estén indexadas puede acelerar las consultas.\n",
        "\n",
        "4. **Almacenamiento en Formato Eficiente**:\n",
        "    - **Parquet**: Continuar utilizando el formato Parquet, ya que es altamente eficiente para el almacenamiento y recuperación de datos. Considerar el particionado de archivos Parquet por fecha o usuario para mejorar la eficiencia de las consultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrtNQf2rEfuD",
        "outputId": "5c78cd4b-32eb-4613-b544-1224b430b0ac"
      },
      "outputs": [],
      "source": [
        "results = q1_time(TEMP_FILE_PATH)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de Memoria en Archivos Parquet: Función `q1_memory`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Escalabilidad**: Spark está diseñado para procesar grandes volúmenes de datos en clústeres distribuidos, lo que permite manejar datasets que exceden la capacidad de memoria de una sola máquina.\n",
        "    - **Eficiencia**: Spark realiza muchas optimizaciones internas, como la ejecución en memoria y la reutilización de datos intermedios, lo que puede acelerar significativamente el procesamiento de datos grandes.\n",
        "\n",
        "2. **Procesamiento Distribuido**:\n",
        "    - **`mapPartitions`**: Utiliza `mapPartitions` para procesar particiones de datos, lo que permite manejar eficientemente los datos distribuidos en múltiples nodos del clúster.\n",
        "    - **`reduceByKey`**: Agrega los resultados de cada partición eficientemente utilizando `reduceByKey`, lo que es esencial para combinar grandes volúmenes de datos distribuidos.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, la arquitectura de Spark permite escalar horizontalmente, añadiendo más nodos al clúster para manejar la carga adicional. Además, hay varias estrategias que se pueden implementar para mejorar aún más la eficiencia y la escalabilidad:\n",
        "\n",
        "1. **Ajustes de Configuración de Spark**:\n",
        "    - **Configuración de Recursos**: Ajustar la cantidad de memoria y núcleos asignados a cada ejecutor en Spark puede mejorar el rendimiento.\n",
        "    - **Persistencia de Datos**: Utilizar persistencia (`cache` o `persist`) para datos intermedios que se reutilizan en múltiples operaciones.\n",
        "\n",
        "2. **Particionado de Datos**:\n",
        "    - **Particionado Eficiente**: Asegurarse de que los datos estén particionados de manera eficiente para equilibrar la carga de trabajo entre los nodos del clúster.\n",
        "    - **Reparticionamiento**: Usar reparticionamiento (`repartition`) cuando sea necesario para optimizar la distribución de datos antes de operaciones intensivas en memoria.\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Filtro Previo**: Aplicar filtros tempranos para reducir la cantidad de datos procesados en etapas posteriores.\n",
        "    - **Proyección de Columnas**: Seleccionar solo las columnas necesarias para el análisis, reduciendo el volumen de datos cargados en memoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = q1_memory(TEMP_FILE_PATH)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHYuMjMbAytU"
      },
      "source": [
        "## Los top 10 emojis más usados con su respectivo conteo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Consultas de Tiempo en BigQuery: Función `q2_time`\n",
        "\n",
        "La función `q2_time` se utiliza para realizar consultas en BigQuery, extrayendo los 10 emojis más frecuentes en los tweets. Este enfoque aprovecha las capacidades de procesamiento masivo y escalable de BigQuery para manejar grandes volúmenes de datos de manera eficiente.\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de BigQuery**:\n",
        "    - **Procesamiento Escalable**: BigQuery está diseñado para ejecutar consultas SQL a gran escala sobre grandes volúmenes de datos de manera rápida y eficiente.\n",
        "    - **Facilidad de Uso**: La integración con Python permite ejecutar consultas SQL directamente desde el código, facilitando el análisis y manipulación de datos.\n",
        "\n",
        "2. **Optimización Temporal y de Memoria**:\n",
        "    - **Consultas SQL Optimizadas**: La consulta SQL está optimizada para extraer y contar emojis en los tweets, utilizando funciones avanzadas como `REGEXP_EXTRACT_ALL` y `UNNEST` para manejar datos complejos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTyFWu-yGhBk",
        "outputId": "0648fcbf-d0f1-48e7-a49d-01005ab781ef"
      },
      "outputs": [],
      "source": [
        "results = q2_time(TABLE_NAME)\n",
        "print(f\"Memory Optimization - Results: {results}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Consultas de Memoria en Archivos Parquet: Función `q2_memory`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Procesamiento Distribuido**: Spark permite distribuir el procesamiento de datos en múltiples nodos, lo que facilita el manejo de grandes volúmenes de datos de manera eficiente.\n",
        "    - **Facilidad de Uso**: La interfaz de Spark en Python ofrece una API intuitiva para realizar operaciones complejas en grandes datasets.\n",
        "\n",
        "2. **Optimización de Tiempo y Memoria**:\n",
        "    - **Expresiones Regulares**: Utiliza expresiones regulares para extraer emojis de los tweets de manera eficiente, reduciendo el tiempo de procesamiento.\n",
        "    - **Explode y GroupBy**: Utiliza operaciones como `explode` y `groupBy` para transformar los datos y calcular la frecuencia de cada emoji de manera efectiva.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = q2_memory(TEMP_FILE_PATH)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0l1sV2AytU"
      },
      "source": [
        "## El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de Tiempo en Archivos Parquet: Función `q3_time`\n",
        "\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Eficiencia en el Procesamiento de Datos**:\n",
        "    - **ParquetDataset**: Utiliza la clase `ParquetDataset` de PyArrow para leer fragmentos de archivos Parquet de manera eficiente, minimizando la carga en la memoria.\n",
        "    - **Procesamiento Distribuido**: Procesa cada fragmento de Parquet de forma independiente, lo que permite aprovechar la capacidad de procesamiento paralelo y distribuido.\n",
        "\n",
        "2. **Manejo de Texto**:\n",
        "    - **Función de Extracción de Menciones**: La función `extract_mentions` utiliza expresiones regulares para extraer menciones de texto de manera eficiente, lo que facilita el análisis de texto no estructurado.\n",
        "    - **Conteo de Menciones**: Utiliza un contador para mantener un registro de la frecuencia de cada mención, lo que simplifica el proceso de identificar las menciones más comunes.\n",
        "\n",
        "3. **Escalabilidad y Rendimiento**:\n",
        "    - **Optimización de Memoria**: Procesa los datos de manera incremental, lo que reduce la carga en la memoria y permite manejar conjuntos de datos de gran tamaño de manera eficiente.\n",
        "    - **Complejidad Temporal**: Utiliza técnicas de programación eficientes para minimizar el tiempo de ejecución, lo que garantiza un rendimiento óptimo incluso para grandes volúmenes de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = q3_time(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Consultas de Memoria en BigQuery: Función `q3_memory`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Eficiencia en el Procesamiento de Datos**:\n",
        "    - **BigQuery como Servicio de Análisis**: Utiliza BigQuery como un servicio de análisis gestionado que ofrece una alta escalabilidad y rendimiento para procesar grandes volúmenes de datos de manera eficiente.\n",
        "    - **Consulta SQL Optimizada**: La consulta SQL está optimizada para contar los nombres de usuario más mencionados de manera eficiente, aprovechando las funciones integradas de BigQuery para manipular y analizar datos estructurados.\n",
        "\n",
        "2. **Manejo de Datos Estructurados**:\n",
        "    - **Extracción de Nombres de Usuario**: Utiliza expresiones regulares en la consulta SQL para extraer y contar los nombres de usuario mencionados en los tweets, lo que simplifica el proceso de identificación de los usuarios más mencionados.\n",
        "\n",
        "3. **Complejidad Temporal y Escalabilidad**:\n",
        "    - **Tiempo de Ejecución Eficiente**: La consulta está diseñada para minimizar el tiempo de ejecución, lo que garantiza un procesamiento rápido incluso para grandes conjuntos de datos.\n",
        "    - **Escalabilidad Automática**: BigQuery ofrece escalabilidad automática, lo que significa que puede manejar un aumento en el volumen de datos sin necesidad de ajustes manuales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYA_mIq1GkQD",
        "outputId": "4443221f-d8b2-406a-f56b-ebaf4804cb5e"
      },
      "outputs": [],
      "source": [
        "results = q3_memory(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
