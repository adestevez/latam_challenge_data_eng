{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjtZNlRzAytT"
      },
      "source": [
        "# Latam Data Engineer Challenge\n",
        "---\n",
        "## Configuración del Proyecto\n",
        "\n",
        "### Entorno en Google Cloud Platform (GCP)\n",
        "\n",
        "Para solventar el desafío, se configuró un entorno en Google Cloud Platform (GCP) con el nombre `development-latam-de`. Este entorno proporciona la infraestructura necesaria para manejar y procesar los datos del proyecto.\n",
        "\n",
        "![imagen.png](../img/gcp.png)\n",
        "\n",
        "### Arquitectura de Medallón\n",
        "\n",
        "En el contexto de la solución, se implementó una arquitectura de medallón, que organiza los datos en tres niveles distintos:\n",
        "\n",
        "1. **Bronze (Bronce)**: Datos en su forma cruda y original, que se identificará por el inicio `raw`.\n",
        "2. **Silver (Plata)**: Datos refinados y limpios, que se identificará por el inicio `cur`.\n",
        "3. **Gold (Oro)**: Datos altamente curados y optimizados para análisis avanzados, que se identificará por el inicio `mst`.\n",
        "\n",
        "Para este caso particular, se optó por crear tres carpetas dentro de un solo bucket en lugar de tres buckets separados. Esta estructura es ventajosa porque:\n",
        "\n",
        "- **Facilita la organización y gestión de datos**: Cada nivel tiene sus propios datos claramente segregados.\n",
        "- **Mejora la escalabilidad y mantenibilidad**: Es más fácil escalar y mantener el pipeline de datos cuando las etapas están bien definidas.\n",
        "- **Optimiza las políticas de seguridad y acceso**: Permite aplicar diferentes políticas de acceso y seguridad a cada nivel de datos.\n",
        "- **Aumenta el rendimiento de las consultas**: Al trabajar con datos más refinados en cada etapa, las consultas analíticas se vuelven más eficientes.\n",
        "\n",
        "![imagen.png](../img/bucket.png)\n",
        "\n",
        "## Pipeline de Procesamiento e Ingesta de Datos\n",
        "\n",
        "Se definió un pipeline que automatiza el proceso de creación de archivos curados y master. Este pipeline toma los datos crudos en formato `json` desde el bucket que se considera la data `raw`y los procesa adecuadamente.\n",
        "\n",
        "Este archivo se encuentra en `src/create_medallion_pipeline.py`, mientras que el proceso que ejecuta el workflow desde Github Actions `.github/workflows/ingest_pipeline.yml`\n",
        "\n",
        "### Descripción general de `create_medallion_pipeline`\n",
        "\n",
        "#### Creación de Clientes de GCP\n",
        "- Se crean clientes de GCS y BigQuery mediante create_gcp_clients.\n",
        "- En caso de error, se captura y se imprime el mensaje de error.\n",
        "\n",
        "#### Lectura de JSON desde GCS\n",
        "- La función read_json_from_gcs lee un archivo JSON desde GCS y lo decodifica.\n",
        "- Verifica la existencia del archivo y maneja errores durante la lectura.\n",
        "\n",
        "### Conversión de JSON a DataFrame\n",
        "- La función json_lines_to_dataframe convierte las líneas JSON en un DataFrame de Pandas.\n",
        "- Se manejan errores de conversión y se imprime un mensaje de error si ocurre alguno.\n",
        "\n",
        "#### Limpieza y Preparación de Datos\n",
        "- Se renombra columnas para evitar problemas con nombres que contienen puntos y otros caracteres especiales.\n",
        "- Se convierten todas las columnas de tipo object a cadenas de texto.\n",
        "- Almacenamiento en Formato Parquet Los datos se guardan en formato Parquet, un formato de archivo columnar eficiente para análisis.\n",
        "- Se maneja la subida de los archivos Parquet a GCS y se capturan posibles errores durante el proceso.\n",
        "- Selección de Columnas y Creación de Archivo Final\n",
        "- Se seleccionan columnas específicas del DataFrame para crear una versión \"Gold\" de los datos. Esta versión final también se guarda y se sube a GCS en formato Parquet.\n",
        "\n",
        "### Descripción general de `ingest_pipeline.yml`\n",
        "\n",
        "El pipeline se ejecuta manualmente utilizando GitHub Actions, lo cual ofrece varias ventajas:\n",
        "\n",
        "- **Automatización**: Garantiza que los procesos de ETL (Extracción, Transformación y Carga) sean automáticos y repetibles.\n",
        "- **Facilidad de uso**: GitHub Actions permite configurar y ejecutar workflows de manera sencilla.\n",
        "\n",
        "#### Activación Manual\n",
        "- El flujo de trabajo solo se activa cuando se dispara manualmente desde la interfaz de usuario de GitHub.\n",
        "\n",
        "#### Configuración del Entorno\n",
        "- Se configura un entorno de ejecución en Ubuntu usando la versión específica de Python (3.11).\n",
        "\n",
        "#### Instalación de Dependencias\n",
        "- Se instalan las dependencias del proyecto especificadas en el archivo requirements.txt utilizando pip.\n",
        "\n",
        "#### Ejecución del Script\n",
        "- Se ejecuta el script de Python create_medallion_pipeline.py ubicado en la carpeta src.\n",
        "\n",
        "A conitnuación se presentan como se ven las salidas dentro de Github Actions\n",
        "\n",
        "![imagen.png](../img/ingesta.png)\n",
        "\n",
        "El tiempo estimado de ejecución es de 47 segundos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrZDBPJrAytU"
      },
      "source": [
        "## Configuración del Entorno Colab\n",
        "\n",
        "Para configurar el entorno de Google Colab para el proyecto `challenge_de_latam`, se ejecutan varios pasos que son cruciales para preparar el entorno de desarrollo. A continuación, se explican cada uno de estos pasos y su propósito.\n",
        "\n",
        "### Montaje de Google Drive\n",
        "Primero, se monta Google Drive en el entorno de Colab. Esto permite acceder a los archivos almacenados en Google Drive directamente desde el entorno Colab que previamente debieron ser clonados como se menciona en el README.md, facilitando la carga y manipulación de datos, así como el acceso a scripts y recursos necesarios para el proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl_OcLNmabmr",
        "outputId": "83828119-127a-4d8b-e2d3-0ec8f2e3cac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb8kaBD8abmr"
      },
      "source": [
        "### Navegación al Directorio del Proyecto\n",
        "\n",
        "Se cambia el directorio de trabajo al repositorio del proyecto en Google Drive. Esto asegura que todos los comandos y scripts que se ejecuten a continuación se realicen en el contexto correcto del proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxJ9XTxmabmr",
        "outputId": "eb965943-8193-4505-8d26-2a125664c493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/repos/latam_challenge_data_eng\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/repos/latam_challenge_data_eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGUrOUkrabms"
      },
      "source": [
        "### Instalación de Apache Spark y Dependencias del Proyecto\n",
        "\n",
        "Se ejecuta un script bash para instalar Apache Spark. Este paso garantiza que Spark esté disponible en el entorno de Colab.\n",
        "Seguido de ello, se instalan todas las dependencias necesarias especificadas en el archivo requirements.txt. Este archivo contiene todas las librerías y paquetes de Python que el proyecto necesita para ejecutarse correctamente.\n",
        "\n",
        "### Reinicio del Entorno de Colab\n",
        "\n",
        "Finalmente, se reinicia el entorno de Colab matando el proceso actual. Este paso es importante porque algunos cambios, como la instalación de Spark y otras dependencias, requieren un reinicio del kernel para aplicarse correctamente. Reiniciar el entorno garantiza que todas las instalaciones y configuraciones se apliquen de manera efectiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7_dompBIJx",
        "outputId": "8fd6ab7f-66d9-44a1-a570-b7bc80d4ac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 1s (168 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Requirement already satisfied: memory-profiler==0.61.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.61.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage==2.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.21.0)\n",
            "Requirement already satisfied: emoji==2.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.12.1)\n",
            "Requirement already satisfied: pyarrow==16.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (16.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler==0.61.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.7.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery==3.21.0->-r requirements.txt (line 5)) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji==2.12.1->-r requirements.txt (line 6)) (4.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.48.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!bash ./install_spark.sh\n",
        "!pip install -r requirements.txt\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ocq4W5TE3t3"
      },
      "source": [
        "# Importación de librerías\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YmJkyKuDE90u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import findspark\n",
        "from google.cloud import storage, bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import userdata\n",
        "\n",
        "# Inicializar findspark antes de cualquier importación relacionada con Spark\n",
        "findspark.init()\n",
        "\n",
        "# Añadir el directorio del proyecto a la ruta del sistema\n",
        "sys.path.append('/content/drive/MyDrive/repos/latam_challenge_data_eng/src')\n",
        "\n",
        "# Importar los módulos específicos del proyecto\n",
        "from q1_time import q1_time\n",
        "from q1_memory import q1_memory\n",
        "from q2_time import q2_time\n",
        "from q2_memory import q2_memory\n",
        "from q3_time import q3_time\n",
        "from q3_memory import q3_memory\n",
        "from gcp_client import create_gcp_clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUE4WxDKEqy7"
      },
      "source": [
        "# Obtención de los secretos almacenados en Google Colab\n",
        "---\n",
        "Se obtienen los secretos almacenados en Google Colab utilizando el módulo `userdata`. Este proceso es crucial para acceder de manera segura a las credenciales y otros secretos necesarios para interactuar con los servicios de Google Cloud Platform (GCP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eVFv67-HFjcd"
      },
      "outputs": [],
      "source": [
        "gcp_service_account = userdata.get('gcp_service_account')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTM342e4abmt"
      },
      "source": [
        "Es importante mencionar que para la ejecución del presente proceo en Github Actions, se ha considerado el uso de Secrets dentro su respectiva plataforma.\n",
        "![imagen.png](../img/secreto.png)\n",
        "\n",
        "El mismo se obtiene en el Workflow `data_aggregation_pipeline.yml` como se muestra a continuación\n",
        "![imagen.png](../img/entorno.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY1UT8PRabmt"
      },
      "source": [
        "## Cargar las Credenciales desde la Cadena JSON\n",
        "---\n",
        "En esta sección, se cargan las credenciales de GCP desde una cadena JSON y se crean clientes autenticados para Google Cloud Storage y BigQuery. Este proceso es fundamental para interactuar de manera segura y autorizada con los servicios de GCP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xEDYj70jFhki"
      },
      "outputs": [],
      "source": [
        "# Cargar las credenciales desde la cadena JSON\n",
        "credentials_info = json.loads(gcp_service_account)\n",
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "    credentials_info)\n",
        "# Crear el cliente de almacenamiento con las credenciales cargadas\n",
        "storage_client = storage.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])\n",
        "# Crear el cliente de BigQuery con las credenciales cargadas\n",
        "bigquery_client = bigquery.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6oHSquIabmt"
      },
      "source": [
        "## Operaciones con el Bucket de Google Cloud Storage\n",
        "---\n",
        "\n",
        "Se realizan varias operaciones con un bucket específico de Google Cloud Storage, incluyendo la descarga de un archivo Parquet a una ubicación temporal en el entorno de ejecución. Las mismas difieren del abordaje definido para ejecutar con el fichero `challenge.py` por la manera de explicación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5l3-JZjb7b6",
        "outputId": "a60aa1fa-6719-40da-ae25-f70cd517f878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket dl-latam-dev está listo para usar.\n"
          ]
        }
      ],
      "source": [
        "BUCKET_NAME = 'dl-latam-dev'\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "# Operaciones con el bucket\n",
        "print(f\"Bucket {BUCKET_NAME} está listo para usar.\")\n",
        "\n",
        "# Definir el archivo Parquet y la ruta temporal\n",
        "FILE_NAME = 'mst_farmers_tweets.parquet'\n",
        "TEMP_FILE_PATH = '/tmp/mst_farmers_tweets.parquet'\n",
        "\n",
        "# Descargar el archivo Parquet a un archivo temporal\n",
        "blob = bucket.blob(FILE_NAME)\n",
        "blob.download_to_filename(TEMP_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN49ea4AytU"
      },
      "source": [
        "## Question 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFt_zZVnabmt"
      },
      "source": [
        "### Análisis de Tiempo en Archivos Parquet: Función `q1_time`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Pandas**:\n",
        "    - **Facilidad de Uso**: Pandas proporciona una interfaz sencilla y poderosa para la manipulación de datos, lo que facilita la carga y transformación de grandes datasets.\n",
        "    - **Eficiencia**: La función `pd.read_parquet` es eficiente en la carga de archivos Parquet, que están optimizados para consultas rápidas y almacenamiento compacto.\n",
        "\n",
        "2. **Optimización Temporal**:\n",
        "    - **Conversión de Fechas**: La conversión de la columna de fechas a objetos datetime y la extracción de solo la parte de la fecha permite un análisis más rápido y preciso de las fechas de los tweets.\n",
        "    - **Contado de Frecuencia**: El uso de `value_counts` en Pandas es altamente optimizado para contar frecuencias, permitiendo identificar rápidamente las fechas con más actividad.\n",
        "\n",
        "3. **Análisis de Usuarios**:\n",
        "    - **Identificación del Usuario Top**: Para cada una de las fechas más activas, se utiliza `value_counts` nuevamente para identificar el usuario con más tweets. Este enfoque es eficiente y directo.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, hay varias estrategias que se pueden implementar para mantener la eficiencia y escalabilidad del análisis:\n",
        "\n",
        "1. **Procesamiento en Chunks**:\n",
        "    - **Lectura Parcial de Datos**: Pandas permite la lectura de archivos en chunks, lo que reduce el uso de memoria y permite procesar datasets más grandes que la memoria disponible.\n",
        "\n",
        "    ```python\n",
        "    for chunk in pd.read_parquet(file_path, chunksize=100000):\n",
        "        # Procesar cada chunk individualmente\n",
        "    ```\n",
        "\n",
        "2. **Uso de Dask**:\n",
        "    - **Distribución de Carga**: Dask es una biblioteca que extiende Pandas para trabajar con datos distribuidos y en paralelo. Usar Dask en lugar de Pandas puede mejorar significativamente el rendimiento con grandes volúmenes de datos.\n",
        "\n",
        "    ```python\n",
        "    import dask.dataframe as dd\n",
        "    df = dd.read_parquet(file_path)\n",
        "    ```\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Indexación y Filtrado**: Asegurar que las columnas utilizadas para filtrar y agrupar (como la columna de fechas) estén indexadas puede acelerar las consultas.\n",
        "\n",
        "4. **Almacenamiento en Formato Eficiente**:\n",
        "    - **Parquet**: Continuar utilizando el formato Parquet, ya que es altamente eficiente para el almacenamiento y recuperación de datos. Considerar el particionado de archivos Parquet por fecha o usuario para mejorar la eficiencia de las consultas.\n",
        "\n",
        "###  Descripción General\n",
        "Este módulo contiene una función llamada q1_time diseñada para realizar análisis de tiempo en un archivo Parquet. Utiliza pandas para cargar y manipular datos, y devuelve una lista de tuplas que contiene las fechas más frecuentes y el usuario que realizó más tweets en cada fecha.\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q1_time\n",
        "\n",
        "Es la función principal que realiza el análisis de tiempo en el archivo Parquet.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo Parquet como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Carga de Datos\n",
        "\n",
        "Utiliza pd.read_parquet para cargar el archivo Parquet en un DataFrame de pandas.\n",
        "- Manipulación de Datos\n",
        "\n",
        "Convierte la columna de fechas a formato datetime y extrae solo la fecha.\n",
        "Cuenta el número de tweets por fecha y encuentra las 10 fechas más frecuentes.\n",
        "- Análisis de Usuarios por Fecha\n",
        "\n",
        "Para cada fecha más frecuente, encuentra el usuario que realizó más tweets.\n",
        "Almacena la fecha y el usuario en una lista de resultados.\n",
        "- Medición de Tiempo de Ejecución\n",
        "\n",
        "Mide el tiempo de ejecución total de la función y lo imprime.\n",
        "\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "!![imagen.png](../img/tiempo1.png)\n",
        "\n",
        "\n",
        "Siendo un tiempo de 0,80 segundos y 473MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrtNQf2rEfuD",
        "outputId": "eeefb5fe-b8c1-4a00-c6b4-2a1df9492ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 847, in enable\n",
            "    sys.settrace(self.trace_memory_usage)\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 850, in disable\n",
            "    sys.settrace(self._original_trace_function)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  1.7016263008117676\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q1_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    11    199.4 MiB    199.4 MiB           1   @profile\n",
            "    12                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    13                                             \"\"\"\n",
            "    14                                             Realiza análisis de tiempo en un archivo Parquet.\n",
            "    15                                         \n",
            "    16                                             Parameters:\n",
            "    17                                                 file_path (str): La ruta del archivo Parquet a analizar.\n",
            "    18                                         \n",
            "    19                                             Returns:\n",
            "    20                                                 List[Tuple[datetime.date, str]]: Una lista de tuplas que contiene las fechas\n",
            "    21                                                 más frecuentes y el usuario que más tweets ha realizado en cada fecha.\n",
            "    22                                         \n",
            "    23                                             Raises:\n",
            "    24                                                 FileNotFoundError: Si no se encuentra el archivo especificado en file_path.\n",
            "    25                                             \"\"\"\n",
            "    26    199.4 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    27                                         \n",
            "    28    199.4 MiB      0.0 MiB           1       try:\n",
            "    29                                                 # Cargar el archivo Parquet en un DataFrame de pandas\n",
            "    30    476.7 MiB    277.3 MiB           1           df = pd.read_parquet(file_path)\n",
            "    31                                             except FileNotFoundError as exc:\n",
            "    32                                                 raise FileNotFoundError(f\"El archivo especificado no se encuentra: {file_path}\") from exc\n",
            "    33                                         \n",
            "    34                                             # Verificar si el DataFrame está vacío\n",
            "    35    476.7 MiB      0.0 MiB           1       if df.empty:\n",
            "    36                                                 print(\"El DataFrame está vacío. No se pueden realizar análisis.\")\n",
            "    37                                                 return []\n",
            "    38                                         \n",
            "    39                                             # Convertir la columna 'date' a datetime y extraer solo la fecha\n",
            "    40    475.0 MiB     -1.7 MiB           1       df['date'] = pd.to_datetime(df['date']).dt.date\n",
            "    41                                         \n",
            "    42                                             # Contar el número de tweets por fecha\n",
            "    43    475.0 MiB      0.0 MiB           1       date_counts = df['date'].value_counts().nlargest(10)\n",
            "    44                                         \n",
            "    45                                             # Para cada una de las fechas top, encontrar el usuario con más tweets\n",
            "    46    475.0 MiB      0.0 MiB           1       results = []\n",
            "    47    476.4 MiB   -344.6 MiB          11       for date in date_counts.index:\n",
            "    48    475.0 MiB   -344.6 MiB          10           user_counts = df[df['date'] == date]['user_username'].value_counts()\n",
            "    49    476.4 MiB   -331.1 MiB          10           top_user = user_counts.idxmax()\n",
            "    50    476.4 MiB   -344.6 MiB          10           results.append((date, top_user))\n",
            "    51                                         \n",
            "    52    423.5 MiB    -52.9 MiB           1       end_time = time.time()\n",
            "    53    423.5 MiB      0.0 MiB           1       execution_time = end_time - start_time\n",
            "    54    423.5 MiB      0.0 MiB           1       print(\"Execution Time: \", execution_time)\n",
            "    55    423.5 MiB      0.0 MiB           1       return results\n",
            "\n",
            "\n",
            "Time Optimization - Results: [(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
          ]
        }
      ],
      "source": [
        "results = q1_time(TEMP_FILE_PATH)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgmcKklcabmu"
      },
      "source": [
        "### Análisis de Memoria en Archivos Parquet: Función `q1_memory`\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Escalabilidad**: Spark está diseñado para procesar grandes volúmenes de datos en clústeres distribuidos, lo que permite manejar datasets que exceden la capacidad de memoria de una sola máquina.\n",
        "    - **Eficiencia**: Spark realiza muchas optimizaciones internas, como la ejecución en memoria y la reutilización de datos intermedios, lo que puede acelerar significativamente el procesamiento de datos grandes.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, la arquitectura de Spark permite escalar horizontalmente, añadiendo más nodos al clúster para manejar la carga adicional. Además, hay varias estrategias que se pueden implementar para mejorar aún más la eficiencia y la escalabilidad:\n",
        "\n",
        "1. **Ajustes de Configuración de Spark**:\n",
        "    - **Configuración de Recursos**: Ajustar la cantidad de memoria y núcleos asignados a cada ejecutor en Spark puede mejorar el rendimiento.\n",
        "    - **Persistencia de Datos**: Utilizar persistencia (`cache` o `persist`) para datos intermedios que se reutilizan en múltiples operaciones.\n",
        "\n",
        "2. **Particionado de Datos**:\n",
        "    - **Particionado Eficiente**: Asegurarse de que los datos estén particionados de manera eficiente para equilibrar la carga de trabajo entre los nodos del clúster.\n",
        "    - **Reparticionamiento**: Usar reparticionamiento (`repartition`) cuando sea necesario para optimizar la distribución de datos antes de operaciones intensivas en memoria.\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Filtro Previo**: Aplicar filtros tempranos para reducir la cantidad de datos procesados en etapas posteriores.\n",
        "    - **Proyección de Columnas**: Seleccionar solo las columnas necesarias para el análisis, reduciendo el volumen de datos cargados en memoria.\n",
        "\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q1_memory diseñada para realizar análisis de memoria en un archivo Parquet utilizando PySpark. La función utiliza SparkSession para leer y procesar datos grandes de manera distribuida, y devuelve una lista de tuplas que contiene las fechas más frecuentes y el usuario que realizó más tweets en cada fecha.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q1_memory\n",
        "- Creación de Sesión Spark\n",
        "\n",
        "Crea una sesión de Spark utilizando SparkSession.builder.\n",
        "- Lectura de Datos\n",
        "\n",
        "Utiliza Spark para leer el archivo Parquet en un DataFrame de Spark.\n",
        "-Manipulación y Filtrado de Datos\n",
        "\n",
        "Convierte la columna de fechas a formato de fecha utilizando to_date.\n",
        "Calcula el número de tweets por fecha y ordena por frecuencia.\n",
        "Filtra el DataFrame original para incluir solo las fechas más frecuentes.\n",
        "- Análisis de Usuarios por Fecha\n",
        "\n",
        "Agrupa los datos por fecha y usuario, y calcula el número de tweets por usuario en cada fecha.\n",
        "Utiliza funciones de ventana para obtener el usuario con más tweets por fecha.\n",
        "- Recogida de Resultados\n",
        "\n",
        "Recoge los resultados en una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "Es la función principal que realiza el análisis de memoria en el archivo Parquet utilizando PySpark.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria1a.png)\n",
        "![imagen.png](../img/memoria1.png)\n",
        "\n",
        "Siendo un tiempo de 10,98 segundos y 250,3MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QidO-RbAabmu",
        "outputId": "8fe7eb04-89e7-43aa-8ef4-0af3d9494c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  24.441723585128784\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q1_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    13    343.5 MiB    343.5 MiB           1   @profile\n",
            "    14                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    15                                             \"\"\"\n",
            "    16                                             Realiza análisis de memoria en un archivo Parquet.\n",
            "    17                                         \n",
            "    18                                             Parameters:\n",
            "    19                                                 file_path (str): La ruta del archivo Parquet a analizar.\n",
            "    20                                         \n",
            "    21                                             Returns:\n",
            "    22                                                 List[Tuple[datetime.date, str]]: Una lista de tuplas que contiene las fechas\n",
            "    23                                                 más frecuentes y el usuario que más tweets ha realizado en cada fecha.\n",
            "    24                                             \"\"\"\n",
            "    25    343.5 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    26                                         \n",
            "    27                                             # Crear una sesión de Spark\n",
            "    28    254.9 MiB    -88.5 MiB           1       spark = SparkSession.builder.appName(\"OptimizeTime\").getOrCreate()\n",
            "    29                                         \n",
            "    30                                             # Leer el archivo Parquet en un DataFrame de Spark\n",
            "    31    254.9 MiB      0.0 MiB           1       df_spark = spark.read.parquet(file_path)\n",
            "    32                                         \n",
            "    33                                             # Convertir la columna 'date' a fecha\n",
            "    34    254.9 MiB      0.0 MiB           1       df_spark = df_spark.withColumn(\"date\", to_date(col(\"date\")))\n",
            "    35                                         \n",
            "    36                                             # Contar el número de tweets por fecha y ordenar por la frecuencia\n",
            "    37    254.9 MiB      0.0 MiB           2       top_dates_df = (df_spark.groupBy(\"date\")\n",
            "    38    254.9 MiB      0.0 MiB           1                                .agg(count(\"*\").alias(\"count\"))\n",
            "    39    254.9 MiB      0.0 MiB           1                                .orderBy(col(\"count\").desc())\n",
            "    40    254.9 MiB      0.0 MiB           1                                .limit(10))\n",
            "    41                                         \n",
            "    42                                             # Obtener la lista de las fechas más frecuentes\n",
            "    43    254.9 MiB      0.0 MiB          13       top_dates = [row[\"date\"] for row in top_dates_df.collect()]\n",
            "    44                                         \n",
            "    45                                             # Filtrar el DataFrame original para incluir solo las fechas más frecuentes\n",
            "    46    254.9 MiB      0.0 MiB           1       df_filtered = df_spark.filter(col(\"date\").isin(top_dates))\n",
            "    47                                         \n",
            "    48                                             # Contar el número de tweets por usuario en cada fecha\n",
            "    49    254.9 MiB      0.0 MiB           2       user_counts_df = (df_filtered.groupBy(\"date\", \"user_username\")\n",
            "    50    254.9 MiB      0.0 MiB           1                                     .agg(count(\"*\").alias(\"tweet_count\")))\n",
            "    51                                         \n",
            "    52                                             # Usar window function para obtener el usuario con más tweets por cada fecha\n",
            "    53    255.0 MiB      0.0 MiB           1       window_spec = Window.partitionBy(\"date\").orderBy(col(\"tweet_count\").desc())\n",
            "    54    255.0 MiB      0.0 MiB           2       top_users_df = (user_counts_df.withColumn(\"rank\", row_number().over(window_spec))\n",
            "    55    255.0 MiB      0.0 MiB           1                                     .filter(col(\"rank\") == 1)\n",
            "    56    255.0 MiB      0.0 MiB           1                                     .select(\"date\", \"user_username\"))\n",
            "    57                                         \n",
            "    58                                             # Recoger los resultados\n",
            "    59    255.0 MiB      0.0 MiB          13       results = [(row[\"date\"], row[\"user_username\"]) for row in top_users_df.collect()]\n",
            "    60                                         \n",
            "    61    255.0 MiB      0.0 MiB           1       end_time = time.time()\n",
            "    62    255.0 MiB      0.0 MiB           1       execution_time = end_time - start_time\n",
            "    63    255.0 MiB      0.0 MiB           1       print(\"Execution Time: \", execution_time)\n",
            "    64    255.0 MiB      0.0 MiB           1       spark.stop()\n",
            "    65    255.0 MiB      0.0 MiB           1       return results\n",
            "\n",
            "\n",
            "Memory Optimization - Results: [(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria')]\n"
          ]
        }
      ],
      "source": [
        "results = q1_memory(TEMP_FILE_PATH)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHYuMjMbAytU"
      },
      "source": [
        "## Question 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwAFBVFRabmu"
      },
      "source": [
        "### Consultas de Tiempo en BigQuery: Función `q2_time`\n",
        "\n",
        "La función `q2_time` se utiliza para realizar consultas en BigQuery, extrayendo los 10 emojis más frecuentes en los tweets. Este enfoque aprovecha las capacidades de procesamiento masivo y escalable de BigQuery para manejar grandes volúmenes de datos de manera eficiente.\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de BigQuery**:\n",
        "    - **Procesamiento Escalable**: BigQuery está diseñado para ejecutar consultas SQL a gran escala sobre grandes volúmenes de datos de manera rápida y eficiente.\n",
        "    - **Facilidad de Uso**: La integración con Python permite ejecutar consultas SQL directamente desde el código, facilitando el análisis y manipulación de datos.\n",
        "\n",
        "2. **Optimización Temporal y de Memoria**:\n",
        "    - **Consultas SQL Optimizadas**: La consulta SQL está optimizada para extraer y contar emojis en los tweets, utilizando funciones avanzadas como `REGEXP_EXTRACT_ALL` y `UNNEST` para manejar datos complejos.\n",
        "\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q2_time diseñada para realizar consultas de memoria en BigQuery para obtener los 10 emojis más frecuentes en tweets.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q2_time\n",
        "\n",
        "Es la función principal que realiza la consulta en BigQuery.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo en BigQuery como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Creación de Clientes de GCP\n",
        "\n",
        "Utiliza la función create_gcp_clients para crear un cliente de BigQuery.\n",
        "- Construcción y Ejecución de la Consulta\n",
        "\n",
        "Define una consulta SQL para contar emojis en tweets.\n",
        "Ejecuta la consulta utilizando bigquery_client.query.\n",
        "- Procesamiento de Resultados\n",
        "\n",
        "Recoge los resultados de la consulta y los devuelve como una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/tiempo2a.png)\n",
        "![imagen.png](../img/tiempo2b.png)\n",
        "Siendo un tiempo de 0,79 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTyFWu-yGhBk",
        "outputId": "5011624f-9e02-43fd-cd09-82acea0380e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  1.7723379135131836\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q2_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    255.0 MiB    255.0 MiB           1   @profile\n",
            "    10                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza consultas de memoria en BigQuery para obtener los 10 emojis más frecuentes en tweets.\n",
            "    13                                         \n",
            "    14                                             Parameters:\n",
            "    15                                                 file_path (str): La ruta del archivo en BigQuery.\n",
            "    16                                         \n",
            "    17                                             Returns:\n",
            "    18                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene los 10 emojis más frecuentes\n",
            "    19                                                 y su frecuencia en los tweets.\n",
            "    20                                             \"\"\"\n",
            "    21    255.0 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    22                                         \n",
            "    23    255.0 MiB      0.0 MiB           1       try:\n",
            "    24                                                 # Crear clientes de GCP para BigQuery\n",
            "    25    255.0 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    26                                         \n",
            "    27                                                 # Query para contar emojis en tweets\n",
            "    28    255.0 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    29                                                         WITH emojis_table AS (\n",
            "    30                                                           SELECT \n",
            "    31                                                             REGEXP_EXTRACT_ALL(content, r'[\\x{{1F600}}-\\x{{1F64F}}\\x{{1F300}}-\\x{{1F5FF}}\\x{{1F680}}-\\x{{1F6FF}}\\x{{1F1E0}}-\\x{{1F1FF}}]') AS emojis\n",
            "    32                                                           FROM \n",
            "    33    255.0 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    34                                                         )\n",
            "    35                                                         SELECT \n",
            "    36                                                           emoji,\n",
            "    37                                                           COUNT(*) AS count\n",
            "    38                                                         FROM (\n",
            "    39                                                           SELECT \n",
            "    40                                                             emoji\n",
            "    41                                                           FROM \n",
            "    42                                                             emojis_table,\n",
            "    43                                                             UNNEST(emojis) AS emoji\n",
            "    44                                                           WHERE \n",
            "    45                                                             NOT REGEXP_CONTAINS(emoji, r'[\\p{{N}}#]')\n",
            "    46                                                         ) AS filtered_emojis\n",
            "    47                                                         GROUP BY \n",
            "    48                                                           emoji\n",
            "    49                                                         ORDER BY \n",
            "    50                                                           count DESC\n",
            "    51                                                         LIMIT 10\n",
            "    52                                                         \"\"\"\n",
            "    53                                         \n",
            "    54                                                 # Ejecutar la consulta\n",
            "    55    255.0 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    56                                         \n",
            "    57                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    58    255.0 MiB      0.0 MiB           1           results = query_job.result()\n",
            "    59    255.0 MiB      0.0 MiB           1           end_time = time.time()\n",
            "    60    255.0 MiB      0.0 MiB           1           execution_time = end_time - start_time\n",
            "    61    255.0 MiB      0.0 MiB           1           print(\"Execution Time: \", execution_time)\n",
            "    62    255.0 MiB      0.0 MiB          13           return [(row.emoji, row.count) for row in results]\n",
            "    63                                             except Exception as e:\n",
            "    64                                                 print(f\"Error inesperado: {e}\")\n",
            "    65                                                 return []\n",
            "\n",
            "\n",
            "Time Optimization - Results: [('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('🌾', 2363), ('🇮', 2096), ('🇳', 2094), ('🏻', 2080), ('🏽', 1218), ('👇', 1108), ('💚', 1040)]\n"
          ]
        }
      ],
      "source": [
        "TABLE_NAME = \"mst_tweets.mst_farmers_tweets\"\n",
        "results = q2_time(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYVUzrVaabmu"
      },
      "source": [
        "## Consultas de Memoria en Archivos Parquet: Función `q2_memory`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Procesamiento Distribuido**: Spark permite distribuir el procesamiento de datos en múltiples nodos, lo que facilita el manejo de grandes volúmenes de datos de manera eficiente.\n",
        "    - **Facilidad de Uso**: La interfaz de Spark en Python ofrece una API intuitiva para realizar operaciones complejas en grandes datasets.\n",
        "\n",
        "2. **Optimización de Tiempo y Memoria**:\n",
        "    - **Expresiones Regulares**: Utiliza expresiones regulares para extraer emojis de los tweets de manera eficiente, reduciendo el tiempo de procesamiento.\n",
        "    - **Explode y GroupBy**: Utiliza operaciones como `explode` y `groupBy` para transformar los datos y calcular la frecuencia de cada emoji de manera efectiva.\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q2_memory diseñada para realizar consultas de tiempo en un archivo Parquet para obtener los 10 emojis más frecuentes. Utiliza PySpark para manipular y procesar los datos del archivo Parquet y devuelve una lista de tuplas que contiene los emojis más frecuentes y su frecuencia.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q2_memory\n",
        "\n",
        "Es la función principal que realiza la consulta de tiempo en el archivo Parquet.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo Parquet como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Creación de la Sesión Spark\n",
        "- Lectura y Preprocesamiento de Datos\n",
        "- Manipulación y Análisis de Datos\n",
        "\n",
        "Divide las cadenas de emojis en caracteres individuales y los explota para tener un emoji por fila.\n",
        "Cuenta la frecuencia de cada emoji y selecciona los 10 emojis más frecuentes.\n",
        "- Recogida de Resultados\n",
        "Recoge los resultados de la consulta y los devuelve como una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria2a.png)\n",
        "![imagen.png](../img/memoria2b.png)\n",
        "Siendo un tiempo de 4,13 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jQubAvbxabmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e2b02a-d336-454f-ef1c-1c9f5eb2a78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  12.338483095169067\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q2_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    12    255.0 MiB    255.0 MiB           1   @profile\n",
            "    13                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    14                                             \"\"\"\n",
            "    15                                             Realiza consultas de tiempo en un archivo Parquet para obtener \n",
            "    16                                             los 10 emojis más frecuentes.\n",
            "    17                                         \n",
            "    18                                             Parameters:\n",
            "    19                                                 file_path (str): La ruta del archivo Parquet a analizar.\n",
            "    20                                         \n",
            "    21                                             Returns:\n",
            "    22                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene \n",
            "    23                                                 los 10 emojis más frecuentes y su frecuencia.\n",
            "    24                                         \n",
            "    25                                             Raises:\n",
            "    26                                                 FileNotFoundError: Si no se encuentra el archivo especificado en file_path.\n",
            "    27                                             \"\"\"\n",
            "    28    255.0 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    29                                         \n",
            "    30    255.0 MiB      0.0 MiB           1       try:\n",
            "    31                                                 # Crear una sesión de Spark\n",
            "    32    255.0 MiB      0.0 MiB           2           spark = SparkSession.builder \\\n",
            "    33    255.0 MiB      0.0 MiB           1               .appName(\"Consulta de emojis\") \\\n",
            "    34    255.0 MiB      0.0 MiB           1               .getOrCreate()\n",
            "    35                                         \n",
            "    36                                                 # Leer el archivo Parquet en un DataFrame de Spark\n",
            "    37    255.0 MiB      0.0 MiB           1           df = spark.read.parquet(file_path)\n",
            "    38                                         \n",
            "    39                                                 # Verificar si el DataFrame está vacío\n",
            "    40    255.0 MiB      0.0 MiB           1           if df.isEmpty():\n",
            "    41                                                     print(\"El DataFrame está vacío. No se pueden realizar análisis.\")\n",
            "    42                                                     return []\n",
            "    43                                         \n",
            "    44                                                 # Definir la expresión regular para extraer emojis\n",
            "    45    255.0 MiB      0.0 MiB           1           emoji_regex = (\n",
            "    46    255.0 MiB      0.0 MiB           1               \"[\"\n",
            "    47                                                     \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
            "    48                                                     \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
            "    49                                                     \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
            "    50                                                     \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
            "    51                                                     \"]\"\n",
            "    52                                                 )\n",
            "    53                                                 # Reemplazar todo lo que no sea un emoji con un espacio vacío\n",
            "    54    255.0 MiB      0.0 MiB           2           df_cleaned = df.withColumn(\"cleaned_content\",\n",
            "    55    255.0 MiB      0.0 MiB           1                                      regexp_replace(col(\"content\"), f\"[^{emoji_regex}]\", \"\"))\n",
            "    56                                                 # Dividir la cadena en una lista de caracteres individuales\n",
            "    57    255.0 MiB      0.0 MiB           2           df_split = df_cleaned.withColumn(\"emoji_list\",\n",
            "    58    255.0 MiB      0.0 MiB           1                                            split(col(\"cleaned_content\"), \"\"))\n",
            "    59                                         \n",
            "    60                                                 # Explode para tener un emoji por fila\n",
            "    61    255.0 MiB      0.0 MiB           1           exploded_df = df_split.select(explode(col(\"emoji_list\")).alias(\"emoji\"))\n",
            "    62                                         \n",
            "    63                                                 # Filtrar filas vacías que pueden haber sido generadas\n",
            "    64    255.0 MiB      0.0 MiB           1           filtered_df = exploded_df.filter(col(\"emoji\") != \"\")\n",
            "    65                                         \n",
            "    66                                                 # Contar la frecuencia de cada emoji\n",
            "    67    255.0 MiB      0.0 MiB           3           emoji_counts = (filtered_df.groupBy(\"emoji\")\n",
            "    68    255.0 MiB      0.0 MiB           1                                      .count()\n",
            "    69    255.0 MiB      0.0 MiB           1                                      .orderBy(\"count\", ascending=False)\n",
            "    70    255.0 MiB      0.0 MiB           1                                      .limit(10))\n",
            "    71                                         \n",
            "    72                                                 # Recoger los resultados y convertir a lista de tuplas\n",
            "    73    255.0 MiB      0.0 MiB           1           top_10_emojis = emoji_counts.collect()\n",
            "    74    255.0 MiB      0.0 MiB           1           end_time = time.time()\n",
            "    75    255.0 MiB      0.0 MiB           1           execution_time = end_time - start_time\n",
            "    76    255.0 MiB      0.0 MiB           1           print(\"Execution Time: \", execution_time)\n",
            "    77                                                 # Cerrar la sesión de Spark\n",
            "    78    255.0 MiB      0.0 MiB           1           spark.stop()\n",
            "    79                                         \n",
            "    80    255.0 MiB      0.0 MiB          13           return [(row['emoji'], row['count']) for row in top_10_emojis]\n",
            "    81                                             except AnalysisException as e:\n",
            "    82                                                 print(f\"Error al leer el archivo Parquet: {str(e)}\")\n",
            "    83                                                 return []\n",
            "    84                                             except FileNotFoundError as e:\n",
            "    85                                                 print(f\"El archivo especificado no se encuentra: {str(e)}\")\n",
            "    86                                                 return []\n",
            "    87                                             except Exception as e:\n",
            "    88                                                 print(f\"Error inesperado: {str(e)}\")\n",
            "    89                                                 return []\n",
            "\n",
            "\n",
            "Memory Optimization - Results: [('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('🌾', 2363), ('🇮', 2096), ('🇳', 2094), ('🏻', 2080), ('🏽', 1218), ('👇', 1108), ('💚', 1040)]\n"
          ]
        }
      ],
      "source": [
        "results = q2_memory(TEMP_FILE_PATH)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pVi8ATuabmu"
      },
      "source": [
        "**NOTA: Es importante identificar que el uso de memoria se mantiene un función del uso de Bigquery y Spark, es importante conocer las respectivas implicaciones de procesar la información en la instancia y hacer uso de servicios externos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0l1sV2AytU"
      },
      "source": [
        "## Question 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQCBq8yjabmu"
      },
      "source": [
        "### Análisis de Tiempo Uso de Bigquery: Función `q3_time`\n",
        "\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "Para el presente caso se considera las mismas ventajas, mencionadas anteriormente. Se espera discutir sobre la manera de la ejecución SQL sobre Bigquery.\n",
        "###  Descripción General\n",
        "\n",
        "Obtención del top 10 de usuarios mencionados, buscando optimizar el tiempo. Mediante el uso de Bigquery sobre python.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q3_time\n",
        "- Creación de Clientes de GCP\n",
        "- Construcción y Ejecución de la Consulta\n",
        "- Procesamiento de Resultados\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/tiempo3a.png)\n",
        "![imagen.png](../img/tiempo3b.png)\n",
        "Siendo un tiempo de 0,73 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTv451oAabmu",
        "outputId": "1e83441d-344f-4445-b3fd-7c4802ed3439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  1.7576267719268799\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q3_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    255.0 MiB    255.0 MiB           1   @profile\n",
            "    10                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza análisis de tiempo en un archivo Parquet para obtener las menciones más comunes.\n",
            "    13                                         \n",
            "    14                                             Parameters:\n",
            "    15                                                 file_path (str): tabla donde se hara la consulta.\n",
            "    16                                         \n",
            "    17                                             Returns:\n",
            "    18                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene las menciones más comunes\n",
            "    19                                                 y su frecuencia.\n",
            "    20                                             \"\"\"\n",
            "    21    255.0 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    22                                         \n",
            "    23    255.0 MiB      0.0 MiB           1       try:\n",
            "    24                                                 # Crear clientes de GCP para BigQuery\n",
            "    25    255.0 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    26                                         \n",
            "    27                                                 # Query para contar nombres de usuario en tweets\n",
            "    28    255.0 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    29                                                     -- Subconsulta para extraer los nombres de usuario y contarlos\n",
            "    30                                                     WITH usernames_table AS (\n",
            "    31                                                         SELECT \n",
            "    32                                                             REGEXP_EXTRACT_ALL(mentionedUsers, r\"'username':\\s*'([^']+)'\") AS usernames\n",
            "    33                                                         FROM \n",
            "    34    255.0 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    35                                                     )\n",
            "    36                                         \n",
            "    37                                                     -- Consulta principal para contar los nombres de usuario y ordenarlos\n",
            "    38                                                     SELECT \n",
            "    39                                                         username,\n",
            "    40                                                         COUNT(*) AS count\n",
            "    41                                                     FROM \n",
            "    42                                                     (\n",
            "    43                                                         SELECT username FROM usernames_table, UNNEST(usernames) AS username\n",
            "    44                                                     ) AS user_data\n",
            "    45                                                     GROUP BY \n",
            "    46                                                         username\n",
            "    47                                                     ORDER BY \n",
            "    48                                                         count DESC\n",
            "    49                                                     LIMIT \n",
            "    50                                                         10;\n",
            "    51                                                 \"\"\"\n",
            "    52                                         \n",
            "    53                                                 # Ejecutar la consulta\n",
            "    54    255.0 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    55                                         \n",
            "    56                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    57    255.0 MiB      0.0 MiB           1           results = query_job.result()\n",
            "    58    255.0 MiB      0.0 MiB           1           end_time = time.time()\n",
            "    59    255.0 MiB      0.0 MiB           1           execution_time = end_time - start_time\n",
            "    60    255.0 MiB      0.0 MiB           1           print(\"Execution Time: \", execution_time)\n",
            "    61    255.1 MiB      0.0 MiB          13           return [(row.username, row.count) for row in results]\n",
            "    62                                             except Exception as e:\n",
            "    63                                                 print(f\"Error inesperado: {e}\")\n",
            "    64                                                 return []\n",
            "\n",
            "\n",
            "Time Optimization - Results: [('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
          ]
        }
      ],
      "source": [
        "results = q3_time(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iXbZgJAabmu"
      },
      "source": [
        "## Consultas de Memoria en BigQuery: Función `q3_memory`\n",
        "\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "Para el presente caso se considera las mismas ventajas, mencionadas anteriormente. Se espera discutir sobre la manera de la ejecución SQL sobre Bigquery.\n",
        "###  Descripción General\n",
        "\n",
        "Obtención del top 10 de usuarios mencionados, buscando optimizar el tiempo. Mediante el uso de Bigquery sobre python.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q3_memory\n",
        "- Creación de Clientes de GCP\n",
        "- Construcción y Ejecución de la Consulta\n",
        "- Procesamiento de Resultados\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria3a.png)\n",
        "![imagen.png](../img/memoria3b.png)\n",
        "\n",
        "Siendo un tiempo de 0,66 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYA_mIq1GkQD",
        "outputId": "763e155e-a17b-4801-a0aa-461074cfe6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  1.8144679069519043\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q3_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    255.1 MiB    255.1 MiB           1   @profile\n",
            "    10                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza consultas de memoria en BigQuery para obtener los \n",
            "    13                                             10 nombres de usuario más mencionados en tweets.\n",
            "    14                                         \n",
            "    15                                             Parameters:\n",
            "    16                                                 file_path (str): La ruta del archivo en BigQuery.\n",
            "    17                                         \n",
            "    18                                             Returns:\n",
            "    19                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene \n",
            "    20                                                 los 10 nombres de usuario más mencionados\n",
            "    21                                                 y su frecuencia en los tweets.\n",
            "    22                                             \"\"\"\n",
            "    23    255.1 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    24                                         \n",
            "    25    255.1 MiB      0.0 MiB           1       try:\n",
            "    26                                                 # Crear clientes de GCP para BigQuery\n",
            "    27    255.1 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    28                                         \n",
            "    29                                                 \n",
            "    30                                         \n",
            "    31                                                 # Query para contar nombres de usuario en tweets\n",
            "    32    255.1 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    33                                                     SELECT \n",
            "    34                                                       username,\n",
            "    35                                                       COUNT(*) AS count\n",
            "    36                                                     FROM (\n",
            "    37                                                       SELECT \n",
            "    38                                                         ARRAY_TO_STRING(REGEXP_EXTRACT_ALL(mentionedUsers, r\"'username':\\s*'([^']+)'\"), ',') AS usernames\n",
            "    39                                                       FROM \n",
            "    40    255.1 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    41                                                       WHERE mentionedUsers != \"None\"\n",
            "    42                                                     ) AS usernames_table\n",
            "    43                                                     CROSS JOIN UNNEST(SPLIT(usernames, ',')) AS username\n",
            "    44                                                     GROUP BY \n",
            "    45                                                       username\n",
            "    46                                                     ORDER BY \n",
            "    47                                                       count DESC\n",
            "    48                                                     LIMIT \n",
            "    49                                                       10;\n",
            "    50                                                 \"\"\"\n",
            "    51                                         \n",
            "    52                                                 # Ejecutar la consulta\n",
            "    53    255.1 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    54                                         \n",
            "    55                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    56    255.1 MiB      0.0 MiB           1           results = query_job.result()\n",
            "    57    255.1 MiB      0.0 MiB           1           end_time = time.time()\n",
            "    58    255.1 MiB      0.0 MiB           1           execution_time = end_time - start_time\n",
            "    59    255.1 MiB      0.0 MiB           1           print(\"Execution Time: \", execution_time)\n",
            "    60    255.1 MiB      0.0 MiB          13           return [(row.username, row.count) for row in results]\n",
            "    61                                             except Exception as e:\n",
            "    62                                                 print(f\"Error inesperado: {e}\")\n",
            "    63                                                 return []\n",
            "\n",
            "\n",
            "Memory Optimization - Results: [('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
          ]
        }
      ],
      "source": [
        "results = q3_memory(TABLE_NAME)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwQ9wylXygL-"
      },
      "source": [
        "A continuación podemos ver una comparativa real del uso de los abordajes SQL sobre Bigquery haciendo uso de las consultas implementadas en el proceso.\n",
        "En la parte izquierda vemos que tiene un tiempo optimizado, respecto a la consulta derecha que optimiza memoria.\n",
        "![imagen.png](../img/métricas.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InaZNvE7ygL-"
      },
      "source": [
        "Consulta 1:\n",
        "Es más rápida debido a la combinación de extracción y concatenación en un solo paso, pero consume más memoria debido a la creación de grandes cadenas concatenadas.\n",
        "\n",
        "- Operación Combinada:\n",
        "\n",
        "La subconsulta combina la extracción de los nombres de usuario y su concatenación en una sola cadena separada por comas. Esta operación puede ser computacionalmente eficiente para crear una estructura de datos única.\n",
        "- Desempaquetado en una sola pasada:\n",
        "\n",
        "El CROSS JOIN UNNEST en la tabla ya procesada permite desempaquetar los nombres en una sola operación, lo que puede reducir el tiempo de procesamiento al evitar múltiples pasos intermedios.\n",
        "\n",
        "- Consumo de Memoria:\n",
        "\n",
        "La concatenación inicial de todos los nombres de usuario en una cadena grande puede consumir más memoria, especialmente si hay muchos nombres de usuario en los datos.\n",
        "\n",
        "\n",
        "Consulta 2:\n",
        "\n",
        "Es más eficiente en términos de memoria al manejar listas de nombres de usuario de manera más directa y separada en pasos distintos, aunque esto puede hacerla un poco más lenta.\n",
        "\n",
        "- Separación de Tareas:\n",
        "\n",
        "La CTE (Common Table Expression) separa claramente la extracción de los nombres de usuario de su desempaquetado. Esta separación puede hacer que el manejo de memoria sea más eficiente, ya que los datos se procesan en pasos distintos y pueden ser liberados o reutilizados entre estos pasos.\n",
        "- UNNEST Directo en Listas:\n",
        "\n",
        "El uso de UNNEST directamente en listas de nombres de usuario evita la concatenación y posterior división, lo cual puede ser menos costoso en términos de memoria, ya que maneja las listas de forma más natural y en su formato original.\n",
        "- Optimización de Memoria:\n",
        "\n",
        "La consulta maneja mejor la memoria al no crear grandes cadenas concatenadas. Al tratar con listas de manera más directa, puede usar menos memoria para almacenar y procesar los datos intermedios.\n",
        "\n",
        "\n",
        "**La diferencia radica en cómo cada consulta maneja la extracción y el desempaquetado de los nombres de usuario, balanceando la eficiencia de tiempo y uso de memoria de maneras diferentes.**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}