{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjtZNlRzAytT"
      },
      "source": [
        "# Latam Data Engineer Challenge\n",
        "---\n",
        "## Configuración del Proyecto\n",
        "\n",
        "### Entorno en Google Cloud Platform (GCP)\n",
        "\n",
        "Para solventar el desafío, se configuró un entorno en Google Cloud Platform (GCP) con el nombre `development-latam-de`. Este entorno proporciona la infraestructura necesaria para manejar y procesar los datos del proyecto.\n",
        "\n",
        "![imagen.png](../img/gcp.png)\n",
        "\n",
        "### Arquitectura de Medallón\n",
        "\n",
        "En el contexto de la solución, se implementó una arquitectura de medallón, que organiza los datos en tres niveles distintos:\n",
        "\n",
        "1. **Bronze (Bronce)**: Datos en su forma cruda y original, que se identificará por el inicio `raw`.\n",
        "2. **Silver (Plata)**: Datos refinados y limpios, que se identificará por el inicio `cur`.\n",
        "3. **Gold (Oro)**: Datos altamente curados y optimizados para análisis avanzados, que se identificará por el inicio `mst`.\n",
        "\n",
        "Para este caso particular, se optó por crear tres carpetas dentro de un solo bucket en lugar de tres buckets separados. Esta estructura es ventajosa porque:\n",
        "\n",
        "- **Facilita la organización y gestión de datos**: Cada nivel tiene sus propios datos claramente segregados.\n",
        "- **Mejora la escalabilidad y mantenibilidad**: Es más fácil escalar y mantener el pipeline de datos cuando las etapas están bien definidas.\n",
        "- **Optimiza las políticas de seguridad y acceso**: Permite aplicar diferentes políticas de acceso y seguridad a cada nivel de datos.\n",
        "- **Aumenta el rendimiento de las consultas**: Al trabajar con datos más refinados en cada etapa, las consultas analíticas se vuelven más eficientes.\n",
        "\n",
        "![imagen.png](../img/bucket.png)\n",
        "\n",
        "## Pipeline de Procesamiento e Ingesta de Datos\n",
        "\n",
        "Se definió un pipeline que automatiza el proceso de creación de archivos curados y master. Este pipeline toma los datos crudos en formato `json` desde el bucket que se considera la data `raw`y los procesa adecuadamente.\n",
        "\n",
        "Este archivo se encuentra en `src/create_medallion_pipeline.py`, mientras que el proceso que ejecuta el workflow desde Github Actions `.github/workflows/ingest_pipeline.yml`\n",
        "\n",
        "### Descripción general de `create_medallion_pipeline`\n",
        "\n",
        "#### Creación de Clientes de GCP\n",
        "- Se crean clientes de GCS y BigQuery mediante create_gcp_clients.\n",
        "- En caso de error, se captura y se imprime el mensaje de error.\n",
        "\n",
        "#### Lectura de JSON desde GCS\n",
        "- La función read_json_from_gcs lee un archivo JSON desde GCS y lo decodifica.\n",
        "- Verifica la existencia del archivo y maneja errores durante la lectura.\n",
        "\n",
        "### Conversión de JSON a DataFrame\n",
        "- La función json_lines_to_dataframe convierte las líneas JSON en un DataFrame de Pandas.\n",
        "- Se manejan errores de conversión y se imprime un mensaje de error si ocurre alguno.\n",
        "\n",
        "#### Limpieza y Preparación de Datos\n",
        "- Se renombra columnas para evitar problemas con nombres que contienen puntos y otros caracteres especiales.\n",
        "- Se convierten todas las columnas de tipo object a cadenas de texto.\n",
        "- Almacenamiento en Formato Parquet Los datos se guardan en formato Parquet, un formato de archivo columnar eficiente para análisis.\n",
        "- Se maneja la subida de los archivos Parquet a GCS y se capturan posibles errores durante el proceso.\n",
        "- Selección de Columnas y Creación de Archivo Final\n",
        "- Se seleccionan columnas específicas del DataFrame para crear una versión \"Gold\" de los datos. Esta versión final también se guarda y se sube a GCS en formato Parquet.\n",
        "\n",
        "### Descripción general de `ingest_pipeline.yml`\n",
        "\n",
        "El pipeline se ejecuta manualmente utilizando GitHub Actions, lo cual ofrece varias ventajas:\n",
        "\n",
        "- **Automatización**: Garantiza que los procesos de ETL (Extracción, Transformación y Carga) sean automáticos y repetibles.\n",
        "- **Facilidad de uso**: GitHub Actions permite configurar y ejecutar workflows de manera sencilla.\n",
        "\n",
        "#### Activación Manual\n",
        "- El flujo de trabajo solo se activa cuando se dispara manualmente desde la interfaz de usuario de GitHub.\n",
        "\n",
        "#### Configuración del Entorno\n",
        "- Se configura un entorno de ejecución en Ubuntu usando la versión específica de Python (3.11).\n",
        "\n",
        "#### Instalación de Dependencias\n",
        "- Se instalan las dependencias del proyecto especificadas en el archivo requirements.txt utilizando pip.\n",
        "\n",
        "#### Ejecución del Script\n",
        "- Se ejecuta el script de Python create_medallion_pipeline.py ubicado en la carpeta src.\n",
        "\n",
        "A conitnuación se presentan como se ven las salidas dentro de Github Actions\n",
        "\n",
        "![imagen.png](../img/ingesta.png)\n",
        "\n",
        "El tiempo estimado de ejecución es de 47 segundos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrZDBPJrAytU"
      },
      "source": [
        "## Configuración del Entorno Colab\n",
        "\n",
        "Para configurar el entorno de Google Colab para el proyecto `challenge_de_latam`, se ejecutan varios pasos que son cruciales para preparar el entorno de desarrollo. A continuación, se explican cada uno de estos pasos y su propósito.\n",
        "\n",
        "### Montaje de Google Drive\n",
        "Primero, se monta Google Drive en el entorno de Colab. Esto permite acceder a los archivos almacenados en Google Drive directamente desde el entorno Colab que previamente debieron ser clonados como se menciona en el README.md, facilitando la carga y manipulación de datos, así como el acceso a scripts y recursos necesarios para el proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl_OcLNmabmr",
        "outputId": "fb987f29-6f99-48c5-c6df-616e3355fd5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb8kaBD8abmr"
      },
      "source": [
        "### Navegación al Directorio del Proyecto\n",
        "\n",
        "Se cambia el directorio de trabajo al repositorio del proyecto en Google Drive. Esto asegura que todos los comandos y scripts que se ejecuten a continuación se realicen en el contexto correcto del proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxJ9XTxmabmr",
        "outputId": "0f0f61f6-7548-40b9-f411-d4a90067d308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/repos/latam_challenge_data_eng\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/repos/latam_challenge_data_eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGUrOUkrabms"
      },
      "source": [
        "### Instalación de Apache Spark y Dependencias del Proyecto\n",
        "\n",
        "Se ejecuta un script bash para instalar Apache Spark. Este paso garantiza que Spark esté disponible en el entorno de Colab.\n",
        "Seguido de ello, se instalan todas las dependencias necesarias especificadas en el archivo requirements.txt. Este archivo contiene todas las librerías y paquetes de Python que el proyecto necesita para ejecutarse correctamente.\n",
        "\n",
        "### Reinicio del Entorno de Colab\n",
        "\n",
        "Finalmente, se reinicia el entorno de Colab matando el proceso actual. Este paso es importante porque algunos cambios, como la instalación de Spark y otras dependencias, requieren un reinicio del kernel para aplicarse correctamente. Reiniciar el entorno garantiza que todas las instalaciones y configuraciones se apliquen de manera efectiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7_dompBIJx",
        "outputId": "819d97d8-a2f5-4aad-a704-8d0f83cde294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,376 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,125 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [110 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.0 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,854 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,084 kB]\n",
            "Fetched 6,926 kB in 2s (3,382 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=6b626c152ec8cc3e78beff6bd4da2c7fc9e8ba835debc50c12b29fc62eb67828\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Collecting memory-profiler==0.61.0 (from -r requirements.txt (line 1))\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage==2.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.21.0)\n",
            "Collecting emoji==2.12.1 (from -r requirements.txt (line 6))\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==16.1.0 (from -r requirements.txt (line 7))\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler==0.61.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.7.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery==3.21.0->-r requirements.txt (line 5)) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji==2.12.1->-r requirements.txt (line 6)) (4.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.48.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.8.0->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Installing collected packages: pyarrow, memory-profiler, emoji\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed emoji-2.12.1 memory-profiler-0.61.0 pyarrow-16.1.0\n"
          ]
        }
      ],
      "source": [
        "!bash ./install_spark.sh\n",
        "!pip install -r requirements.txt\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ocq4W5TE3t3"
      },
      "source": [
        "# Importación de librerías\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YmJkyKuDE90u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import findspark\n",
        "from google.cloud import storage, bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import userdata\n",
        "\n",
        "# Inicializar findspark antes de cualquier importación relacionada con Spark\n",
        "findspark.init()\n",
        "\n",
        "# Añadir el directorio del proyecto a la ruta del sistema\n",
        "sys.path.append('/content/drive/MyDrive/repos/latam_challenge_data_eng/src')\n",
        "\n",
        "# Importar los módulos específicos del proyecto\n",
        "from q1_time import q1_time\n",
        "from q1_memory import q1_memory\n",
        "from q2_time import q2_time\n",
        "from q2_memory import q2_memory\n",
        "from q3_time import q3_time\n",
        "from q3_memory import q3_memory\n",
        "from gcp_client import create_gcp_clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUE4WxDKEqy7"
      },
      "source": [
        "# Obtención de los secretos almacenados en Google Colab\n",
        "---\n",
        "Se obtienen los secretos almacenados en Google Colab utilizando el módulo `userdata`. Este proceso es crucial para acceder de manera segura a las credenciales y otros secretos necesarios para interactuar con los servicios de Google Cloud Platform (GCP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eVFv67-HFjcd"
      },
      "outputs": [],
      "source": [
        "gcp_service_account = userdata.get('gcp_service_account')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTM342e4abmt"
      },
      "source": [
        "Es importante mencionar que para la ejecución del presente proceo en Github Actions, se ha considerado el uso de Secrets dentro su respectiva plataforma.\n",
        "![imagen.png](../img/secreto.png)\n",
        "\n",
        "El mismo se obtiene en el Workflow `data_aggregation_pipeline.yml` como se muestra a continuación\n",
        "![imagen.png](../img/entorno.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY1UT8PRabmt"
      },
      "source": [
        "## Cargar las Credenciales desde la Cadena JSON\n",
        "---\n",
        "En esta sección, se cargan las credenciales de GCP desde una cadena JSON y se crean clientes autenticados para Google Cloud Storage y BigQuery. Este proceso es fundamental para interactuar de manera segura y autorizada con los servicios de GCP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xEDYj70jFhki"
      },
      "outputs": [],
      "source": [
        "# Cargar las credenciales desde la cadena JSON\n",
        "credentials_info = json.loads(gcp_service_account)\n",
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "    credentials_info)\n",
        "# Crear el cliente de almacenamiento con las credenciales cargadas\n",
        "storage_client = storage.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])\n",
        "# Crear el cliente de BigQuery con las credenciales cargadas\n",
        "bigquery_client = bigquery.Client(\n",
        "    credentials=credentials, project=credentials_info['project_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6oHSquIabmt"
      },
      "source": [
        "## Operaciones con el Bucket de Google Cloud Storage\n",
        "---\n",
        "\n",
        "Se realizan varias operaciones con un bucket específico de Google Cloud Storage, incluyendo la descarga de un archivo Parquet a una ubicación temporal en el entorno de ejecución. Las mismas difieren del abordaje definido para ejecutar con el fichero `challenge.py` por la manera de explicación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5l3-JZjb7b6",
        "outputId": "1d74382b-f549-4633-b8a4-fee725d7632e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bucket dl-latam-dev está listo para usar.\n"
          ]
        }
      ],
      "source": [
        "BUCKET_NAME = 'dl-latam-dev'\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "# Operaciones con el bucket\n",
        "print(f\"Bucket {BUCKET_NAME} está listo para usar.\")\n",
        "\n",
        "# Definir el archivo Parquet y la ruta temporal\n",
        "FILE_NAME = 'mst_farmers_tweets.parquet'\n",
        "TEMP_FILE_PATH = '/tmp/mst_farmers_tweets.parquet'\n",
        "\n",
        "# Descargar el archivo Parquet a un archivo temporal\n",
        "blob = bucket.blob(FILE_NAME)\n",
        "blob.download_to_filename(TEMP_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN49ea4AytU"
      },
      "source": [
        "## Question 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFt_zZVnabmt"
      },
      "source": [
        "### Análisis de Tiempo en Archivos Parquet: Función `q1_time`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Pandas**:\n",
        "    - **Facilidad de Uso**: Pandas proporciona una interfaz sencilla y poderosa para la manipulación de datos, lo que facilita la carga y transformación de grandes datasets.\n",
        "    - **Eficiencia**: La función `pd.read_parquet` es eficiente en la carga de archivos Parquet, que están optimizados para consultas rápidas y almacenamiento compacto.\n",
        "\n",
        "2. **Optimización Temporal**:\n",
        "    - **Conversión de Fechas**: La conversión de la columna de fechas a objetos datetime y la extracción de solo la parte de la fecha permite un análisis más rápido y preciso de las fechas de los tweets.\n",
        "    - **Contado de Frecuencia**: El uso de `value_counts` en Pandas es altamente optimizado para contar frecuencias, permitiendo identificar rápidamente las fechas con más actividad.\n",
        "\n",
        "3. **Análisis de Usuarios**:\n",
        "    - **Identificación del Usuario Top**: Para cada una de las fechas más activas, se utiliza `value_counts` nuevamente para identificar el usuario con más tweets. Este enfoque es eficiente y directo.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, hay varias estrategias que se pueden implementar para mantener la eficiencia y escalabilidad del análisis:\n",
        "\n",
        "1. **Procesamiento en Chunks**:\n",
        "    - **Lectura Parcial de Datos**: Pandas permite la lectura de archivos en chunks, lo que reduce el uso de memoria y permite procesar datasets más grandes que la memoria disponible.\n",
        "\n",
        "    ```python\n",
        "    for chunk in pd.read_parquet(file_path, chunksize=100000):\n",
        "        # Procesar cada chunk individualmente\n",
        "    ```\n",
        "\n",
        "2. **Uso de Dask**:\n",
        "    - **Distribución de Carga**: Dask es una biblioteca que extiende Pandas para trabajar con datos distribuidos y en paralelo. Usar Dask en lugar de Pandas puede mejorar significativamente el rendimiento con grandes volúmenes de datos.\n",
        "\n",
        "    ```python\n",
        "    import dask.dataframe as dd\n",
        "    df = dd.read_parquet(file_path)\n",
        "    ```\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Indexación y Filtrado**: Asegurar que las columnas utilizadas para filtrar y agrupar (como la columna de fechas) estén indexadas puede acelerar las consultas.\n",
        "\n",
        "4. **Almacenamiento en Formato Eficiente**:\n",
        "    - **Parquet**: Continuar utilizando el formato Parquet, ya que es altamente eficiente para el almacenamiento y recuperación de datos. Considerar el particionado de archivos Parquet por fecha o usuario para mejorar la eficiencia de las consultas.\n",
        "\n",
        "###  Descripción General\n",
        "Este módulo contiene una función llamada q1_time diseñada para realizar análisis de tiempo en un archivo Parquet. Utiliza pandas para cargar y manipular datos, y devuelve una lista de tuplas que contiene las fechas más frecuentes y el usuario que realizó más tweets en cada fecha.\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q1_time\n",
        "\n",
        "Es la función principal que realiza el análisis de tiempo en el archivo Parquet.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo Parquet como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Carga de Datos\n",
        "\n",
        "Utiliza pd.read_parquet para cargar el archivo Parquet en un DataFrame de pandas.\n",
        "- Manipulación de Datos\n",
        "\n",
        "Convierte la columna de fechas a formato datetime y extrae solo la fecha.\n",
        "Cuenta el número de tweets por fecha y encuentra las 10 fechas más frecuentes.\n",
        "- Análisis de Usuarios por Fecha\n",
        "\n",
        "Para cada fecha más frecuente, encuentra el usuario que realizó más tweets.\n",
        "Almacena la fecha y el usuario en una lista de resultados.\n",
        "- Medición de Tiempo de Ejecución\n",
        "\n",
        "Mide el tiempo de ejecución total de la función y lo imprime.\n",
        "\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "!![imagen.png](../img/tiempo1.png)\n",
        "\n",
        "\n",
        "Siendo un tiempo de 0,80 segundos y 473MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrtNQf2rEfuD",
        "outputId": "e183be5f-5cd3-4d2d-d877-b03648ad3411"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 847, in enable\n",
            "    sys.settrace(self.trace_memory_usage)\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 850, in disable\n",
            "    sys.settrace(self._original_trace_function)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time:  3.986464023590088\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q1_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    11    199.7 MiB    199.7 MiB           1   @profile\n",
            "    12                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    13                                             \"\"\"\n",
            "    14                                             Realiza análisis de tiempo en un archivo Parquet.\n",
            "    15                                         \n",
            "    16                                             Parameters:\n",
            "    17                                                 file_path (str): La ruta del archivo Parquet a analizar.\n",
            "    18                                         \n",
            "    19                                             Returns:\n",
            "    20                                                 List[Tuple[datetime.date, str]]: Una lista de tuplas que contiene las fechas\n",
            "    21                                                 más frecuentes y el usuario que más tweets ha realizado en cada fecha.\n",
            "    22                                         \n",
            "    23                                             Raises:\n",
            "    24                                                 FileNotFoundError: Si no se encuentra el archivo especificado en file_path.\n",
            "    25                                             \"\"\"\n",
            "    26    199.7 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    27                                         \n",
            "    28    199.7 MiB      0.0 MiB           1       try:\n",
            "    29                                                 # Cargar el archivo Parquet en un DataFrame de pandas\n",
            "    30    477.8 MiB    278.1 MiB           1           df = pd.read_parquet(file_path)\n",
            "    31                                             except FileNotFoundError as exc:\n",
            "    32                                                 raise FileNotFoundError(f\"El archivo especificado no se encuentra: {file_path}\") from exc\n",
            "    33                                         \n",
            "    34                                             # Verificar si el DataFrame está vacío\n",
            "    35    477.8 MiB      0.0 MiB           1       if df.empty:\n",
            "    36                                                 print(\"El DataFrame está vacío. No se pueden realizar análisis.\")\n",
            "    37                                                 return []\n",
            "    38                                         \n",
            "    39                                             # Convertir la columna 'date' a datetime y extraer solo la fecha\n",
            "    40    314.6 MiB   -163.1 MiB           1       df['date'] = pd.to_datetime(df['date']).dt.date\n",
            "    41                                         \n",
            "    42                                             # Contar el número de tweets por fecha\n",
            "    43    315.5 MiB      0.8 MiB           1       date_counts = df['date'].value_counts().nlargest(10)\n",
            "    44                                         \n",
            "    45                                             # Para cada una de las fechas top, encontrar el usuario con más tweets\n",
            "    46    315.5 MiB      0.0 MiB           1       results = []\n",
            "    47    315.5 MiB      0.0 MiB          11       for date in date_counts.index:\n",
            "    48    315.5 MiB      0.0 MiB          10           user_counts = df[df['date'] == date]['user_username'].value_counts()\n",
            "    49    315.5 MiB      0.0 MiB          10           top_user = user_counts.idxmax()\n",
            "    50    315.5 MiB      0.0 MiB          10           results.append((date, top_user))\n",
            "    51                                         \n",
            "    52    315.5 MiB      0.0 MiB           1       end_time = time.time()\n",
            "    53    315.5 MiB      0.0 MiB           1       execution_time = end_time - start_time\n",
            "    54    315.5 MiB      0.0 MiB           1       print(\"Execution Time: \", execution_time)\n",
            "    55    315.5 MiB      0.0 MiB           1       return results\n",
            "\n",
            "\n",
            "Time Optimization - Results: [(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
          ]
        }
      ],
      "source": [
        "results = q1_time(TEMP_FILE_PATH)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgmcKklcabmu"
      },
      "source": [
        "### Análisis de Memoria en Archivos Parquet: Función `q1_memory`\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Escalabilidad**: Spark está diseñado para procesar grandes volúmenes de datos en clústeres distribuidos, lo que permite manejar datasets que exceden la capacidad de memoria de una sola máquina.\n",
        "    - **Eficiencia**: Spark realiza muchas optimizaciones internas, como la ejecución en memoria y la reutilización de datos intermedios, lo que puede acelerar significativamente el procesamiento de datos grandes.\n",
        "\n",
        "### Escalabilidad y Manejo de Crecimiento de Datos\n",
        "\n",
        "Si el volumen de datos crece significativamente, la arquitectura de Spark permite escalar horizontalmente, añadiendo más nodos al clúster para manejar la carga adicional. Además, hay varias estrategias que se pueden implementar para mejorar aún más la eficiencia y la escalabilidad:\n",
        "\n",
        "1. **Ajustes de Configuración de Spark**:\n",
        "    - **Configuración de Recursos**: Ajustar la cantidad de memoria y núcleos asignados a cada ejecutor en Spark puede mejorar el rendimiento.\n",
        "    - **Persistencia de Datos**: Utilizar persistencia (`cache` o `persist`) para datos intermedios que se reutilizan en múltiples operaciones.\n",
        "\n",
        "2. **Particionado de Datos**:\n",
        "    - **Particionado Eficiente**: Asegurarse de que los datos estén particionados de manera eficiente para equilibrar la carga de trabajo entre los nodos del clúster.\n",
        "    - **Reparticionamiento**: Usar reparticionamiento (`repartition`) cuando sea necesario para optimizar la distribución de datos antes de operaciones intensivas en memoria.\n",
        "\n",
        "3. **Optimización de Consultas**:\n",
        "    - **Filtro Previo**: Aplicar filtros tempranos para reducir la cantidad de datos procesados en etapas posteriores.\n",
        "    - **Proyección de Columnas**: Seleccionar solo las columnas necesarias para el análisis, reduciendo el volumen de datos cargados en memoria.\n",
        "\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q1_memory diseñada para realizar análisis de memoria en un archivo Parquet utilizando PySpark. La función utiliza SparkSession para leer y procesar datos grandes de manera distribuida, y devuelve una lista de tuplas que contiene las fechas más frecuentes y el usuario que realizó más tweets en cada fecha.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q1_memory\n",
        "- Creación de Sesión Spark\n",
        "\n",
        "Crea una sesión de Spark utilizando SparkSession.builder.\n",
        "- Lectura de Datos\n",
        "\n",
        "Utiliza Spark para leer el archivo Parquet en un DataFrame de Spark.\n",
        "-Manipulación y Filtrado de Datos\n",
        "\n",
        "Convierte la columna de fechas a formato de fecha utilizando to_date.\n",
        "Calcula el número de tweets por fecha y ordena por frecuencia.\n",
        "Filtra el DataFrame original para incluir solo las fechas más frecuentes.\n",
        "- Análisis de Usuarios por Fecha\n",
        "\n",
        "Agrupa los datos por fecha y usuario, y calcula el número de tweets por usuario en cada fecha.\n",
        "Utiliza funciones de ventana para obtener el usuario con más tweets por fecha.\n",
        "- Recogida de Resultados\n",
        "\n",
        "Recoge los resultados en una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "Es la función principal que realiza el análisis de memoria en el archivo Parquet utilizando PySpark.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria1a.png)\n",
        "![imagen.png](../img/memoria1.png)\n",
        "\n",
        "Siendo un tiempo de 10,98 segundos y 250,3MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QidO-RbAabmu",
        "outputId": "48d81275-ba27-4f31-f8d0-ac76a8c67517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time:  24.824762105941772\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q1_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    13    288.5 MiB    288.5 MiB           1   @profile\n",
            "    14                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    15                                             \"\"\"\n",
            "    16                                             Realiza análisis de memoria en un archivo Parquet.\n",
            "    17                                         \n",
            "    18                                             Parameters:\n",
            "    19                                                 file_path (str): La ruta del archivo Parquet a analizar.\n",
            "    20                                         \n",
            "    21                                             Returns:\n",
            "    22                                                 List[Tuple[datetime.date, str]]: Una lista de tuplas que contiene las fechas\n",
            "    23                                                 más frecuentes y el usuario que más tweets ha realizado en cada fecha.\n",
            "    24                                             \"\"\"\n",
            "    25    288.5 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    26                                         \n",
            "    27                                             # Crear una sesión de Spark\n",
            "    28    288.9 MiB      0.3 MiB           1       spark = SparkSession.builder.appName(\"OptimizeTime\").getOrCreate()\n",
            "    29                                         \n",
            "    30                                             # Leer el archivo Parquet en un DataFrame de Spark\n",
            "    31    288.9 MiB      0.0 MiB           1       df_spark = spark.read.parquet(file_path)\n",
            "    32                                         \n",
            "    33                                             # Convertir la columna 'date' a fecha\n",
            "    34    288.9 MiB      0.0 MiB           1       df_spark = df_spark.withColumn(\"date\", to_date(col(\"date\")))\n",
            "    35                                         \n",
            "    36                                             # Contar el número de tweets por fecha y ordenar por la frecuencia\n",
            "    37    288.9 MiB      0.0 MiB           2       top_dates_df = (df_spark.groupBy(\"date\")\n",
            "    38    288.9 MiB      0.0 MiB           1                                .agg(count(\"*\").alias(\"count\"))\n",
            "    39    288.9 MiB      0.0 MiB           1                                .orderBy(col(\"count\").desc())\n",
            "    40    288.9 MiB      0.0 MiB           1                                .limit(10))\n",
            "    41                                         \n",
            "    42                                             # Obtener la lista de las fechas más frecuentes\n",
            "    43    288.9 MiB      0.0 MiB          13       top_dates = [row[\"date\"] for row in top_dates_df.collect()]\n",
            "    44                                         \n",
            "    45                                             # Filtrar el DataFrame original para incluir solo las fechas más frecuentes\n",
            "    46    288.9 MiB      0.0 MiB           1       df_filtered = df_spark.filter(col(\"date\").isin(top_dates))\n",
            "    47                                         \n",
            "    48                                             # Contar el número de tweets por usuario en cada fecha\n",
            "    49    288.9 MiB      0.0 MiB           2       user_counts_df = (df_filtered.groupBy(\"date\", \"user_username\")\n",
            "    50    288.9 MiB      0.0 MiB           1                                     .agg(count(\"*\").alias(\"tweet_count\")))\n",
            "    51                                         \n",
            "    52                                             # Usar window function para obtener el usuario con más tweets por cada fecha\n",
            "    53    288.9 MiB      0.0 MiB           1       window_spec = Window.partitionBy(\"date\").orderBy(col(\"tweet_count\").desc())\n",
            "    54    288.9 MiB      0.0 MiB           2       top_users_df = (user_counts_df.withColumn(\"rank\", row_number().over(window_spec))\n",
            "    55    288.9 MiB      0.0 MiB           1                                     .filter(col(\"rank\") == 1)\n",
            "    56    288.9 MiB      0.0 MiB           1                                     .select(\"date\", \"user_username\"))\n",
            "    57                                         \n",
            "    58                                             # Recoger los resultados\n",
            "    59    288.9 MiB      0.0 MiB          13       results = [(row[\"date\"], row[\"user_username\"]) for row in top_users_df.collect()]\n",
            "    60                                         \n",
            "    61    288.9 MiB      0.0 MiB           1       end_time = time.time()\n",
            "    62    288.9 MiB      0.0 MiB           1       execution_time = end_time - start_time\n",
            "    63    289.1 MiB      0.2 MiB           1       print(\"Execution Time: \", execution_time)\n",
            "    64    289.1 MiB      0.0 MiB           1       spark.stop()\n",
            "    65    289.1 MiB      0.0 MiB           1       return results\n",
            "\n",
            "\n",
            "Memory Optimization - Results: [(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria')]\n"
          ]
        }
      ],
      "source": [
        "results = q1_memory(TEMP_FILE_PATH)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHYuMjMbAytU"
      },
      "source": [
        "## Question 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwAFBVFRabmu"
      },
      "source": [
        "### Consultas de Tiempo en BigQuery: Función `q2_time`\n",
        "\n",
        "La función `q2_time` se utiliza para realizar consultas en BigQuery, extrayendo los 10 emojis más frecuentes en los tweets. Este enfoque aprovecha las capacidades de procesamiento masivo y escalable de BigQuery para manejar grandes volúmenes de datos de manera eficiente.\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de BigQuery**:\n",
        "    - **Procesamiento Escalable**: BigQuery está diseñado para ejecutar consultas SQL a gran escala sobre grandes volúmenes de datos de manera rápida y eficiente.\n",
        "    - **Facilidad de Uso**: La integración con Python permite ejecutar consultas SQL directamente desde el código, facilitando el análisis y manipulación de datos.\n",
        "\n",
        "2. **Optimización Temporal y de Memoria**:\n",
        "    - **Consultas SQL Optimizadas**: La consulta SQL está optimizada para extraer y contar emojis en los tweets, utilizando funciones avanzadas como `REGEXP_EXTRACT_ALL` y `UNNEST` para manejar datos complejos.\n",
        "\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q2_time diseñada para realizar consultas de memoria en BigQuery para obtener los 10 emojis más frecuentes en tweets.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q2_time\n",
        "\n",
        "Es la función principal que realiza la consulta en BigQuery.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo en BigQuery como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Creación de Clientes de GCP\n",
        "\n",
        "Utiliza la función create_gcp_clients para crear un cliente de BigQuery.\n",
        "- Construcción y Ejecución de la Consulta\n",
        "\n",
        "Define una consulta SQL para contar emojis en tweets.\n",
        "Ejecuta la consulta utilizando bigquery_client.query.\n",
        "- Procesamiento de Resultados\n",
        "\n",
        "Recoge los resultados de la consulta y los devuelve como una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/tiempo2a.png)\n",
        "![imagen.png](../img/tiempo2b.png)\n",
        "Siendo un tiempo de 0,79 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTyFWu-yGhBk",
        "outputId": "1eea9d92-3f7d-4ce2-f44a-b1783f2d19a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al crear clientes de GCP: La variable de entorno 'gcp_service_account' no está definida o está vacía.\n",
            "Error inesperado: 'NoneType' object has no attribute 'query'\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q2_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    288.1 MiB    288.1 MiB           1   @profile\n",
            "    10                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza consultas de memoria en BigQuery para obtener los 10 emojis más frecuentes en tweets.\n",
            "    13                                         \n",
            "    14                                             Parameters:\n",
            "    15                                                 file_path (str): La ruta del archivo en BigQuery.\n",
            "    16                                         \n",
            "    17                                             Returns:\n",
            "    18                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene los 10 emojis más frecuentes\n",
            "    19                                                 y su frecuencia en los tweets.\n",
            "    20                                             \"\"\"\n",
            "    21    288.1 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    22                                         \n",
            "    23    288.1 MiB      0.0 MiB           1       try:\n",
            "    24                                                 # Crear clientes de GCP para BigQuery\n",
            "    25    288.1 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    26                                         \n",
            "    27                                                 # Query para contar emojis en tweets\n",
            "    28    288.1 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    29                                                         WITH emojis_table AS (\n",
            "    30                                                           SELECT \n",
            "    31                                                             REGEXP_EXTRACT_ALL(content, r'[\\x{{1F600}}-\\x{{1F64F}}\\x{{1F300}}-\\x{{1F5FF}}\\x{{1F680}}-\\x{{1F6FF}}\\x{{1F1E0}}-\\x{{1F1FF}}]') AS emojis\n",
            "    32                                                           FROM \n",
            "    33    288.1 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    34                                                         )\n",
            "    35                                                         SELECT \n",
            "    36                                                           emoji,\n",
            "    37                                                           COUNT(*) AS count\n",
            "    38                                                         FROM (\n",
            "    39                                                           SELECT \n",
            "    40                                                             emoji\n",
            "    41                                                           FROM \n",
            "    42                                                             emojis_table,\n",
            "    43                                                             UNNEST(emojis) AS emoji\n",
            "    44                                                           WHERE \n",
            "    45                                                             NOT REGEXP_CONTAINS(emoji, r'[\\p{{N}}#]')\n",
            "    46                                                         ) AS filtered_emojis\n",
            "    47                                                         GROUP BY \n",
            "    48                                                           emoji\n",
            "    49                                                         ORDER BY \n",
            "    50                                                           count DESC\n",
            "    51                                                         LIMIT 10\n",
            "    52                                                         \"\"\"\n",
            "    53                                         \n",
            "    54                                                 # Ejecutar la consulta\n",
            "    55    288.1 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    56                                         \n",
            "    57                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    58                                                 results = query_job.result()\n",
            "    59                                                 end_time = time.time()\n",
            "    60                                                 execution_time = end_time - start_time\n",
            "    61                                                 print(\"Execution Time: \", execution_time)\n",
            "    62                                                 return [(row.emoji, row.count) for row in results]\n",
            "    63    288.1 MiB      0.0 MiB           1       except Exception as e:\n",
            "    64    288.1 MiB      0.0 MiB           1           print(f\"Error inesperado: {e}\")\n",
            "    65    288.1 MiB      0.0 MiB           1           return []\n",
            "\n",
            "\n",
            "Memory Optimization - Results: []\n"
          ]
        }
      ],
      "source": [
        "TABLE_NAME = \"mst_tweets.mst_farmers_tweets\"\n",
        "results = q2_time(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYVUzrVaabmu"
      },
      "source": [
        "## Consultas de Memoria en Archivos Parquet: Función `q2_memory`\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "1. **Uso de Apache Spark**:\n",
        "    - **Procesamiento Distribuido**: Spark permite distribuir el procesamiento de datos en múltiples nodos, lo que facilita el manejo de grandes volúmenes de datos de manera eficiente.\n",
        "    - **Facilidad de Uso**: La interfaz de Spark en Python ofrece una API intuitiva para realizar operaciones complejas en grandes datasets.\n",
        "\n",
        "2. **Optimización de Tiempo y Memoria**:\n",
        "    - **Expresiones Regulares**: Utiliza expresiones regulares para extraer emojis de los tweets de manera eficiente, reduciendo el tiempo de procesamiento.\n",
        "    - **Explode y GroupBy**: Utiliza operaciones como `explode` y `groupBy` para transformar los datos y calcular la frecuencia de cada emoji de manera efectiva.\n",
        "###  Descripción General\n",
        "\n",
        "Este módulo contiene una función llamada q2_memory diseñada para realizar consultas de tiempo en un archivo Parquet para obtener los 10 emojis más frecuentes. Utiliza PySpark para manipular y procesar los datos del archivo Parquet y devuelve una lista de tuplas que contiene los emojis más frecuentes y su frecuencia.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q2_memory\n",
        "\n",
        "Es la función principal que realiza la consulta de tiempo en el archivo Parquet.\n",
        "Anotada con @profile para el perfilado de memoria.\n",
        "Toma la ruta del archivo Parquet como entrada y devuelve una lista de tuplas con resultados.\n",
        "- Creación de la Sesión Spark\n",
        "- Lectura y Preprocesamiento de Datos\n",
        "- Manipulación y Análisis de Datos\n",
        "\n",
        "Divide las cadenas de emojis en caracteres individuales y los explota para tener un emoji por fila.\n",
        "Cuenta la frecuencia de cada emoji y selecciona los 10 emojis más frecuentes.\n",
        "- Recogida de Resultados\n",
        "Recoge los resultados de la consulta y los devuelve como una lista de tuplas.\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria2a.png)\n",
        "![imagen.png](../img/memoria2b.png)\n",
        "Siendo un tiempo de 4,13 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQubAvbxabmu"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLa ejecución de celdas con 'Python 3.9.6' requiere el paquete ipykernel.\n",
            "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
            "\u001b[1;31m: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "results = q2_memory(TEMP_FILE_PATH)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pVi8ATuabmu"
      },
      "source": [
        "**NOTA: Es importante identificar que el uso de memoria se mantiene un función del uso de Bigquery y Spark, es importante conocer las respectivas implicaciones de procesar la información en la instancia y hacer uso de servicios externos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0l1sV2AytU"
      },
      "source": [
        "## Question 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQCBq8yjabmu"
      },
      "source": [
        "### Análisis de Tiempo Uso de Bigquery: Función `q3_time`\n",
        "\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "Para el presente caso se considera las mismas ventajas, mencionadas anteriormente. Se espera discutir sobre la manera de la ejecución SQL sobre Bigquery.\n",
        "###  Descripción General\n",
        "\n",
        "Obtención del top 10 de usuarios mencionados, buscando optimizar el tiempo. Mediante el uso de Bigquery sobre python.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q3_time\n",
        "- Creación de Clientes de GCP\n",
        "- Construcción y Ejecución de la Consulta\n",
        "- Procesamiento de Resultados\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/tiempo3a.png)\n",
        "![imagen.png](../img/tiempo3b.png)\n",
        "Siendo un tiempo de 0,73 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTv451oAabmu",
        "outputId": "d3d3124b-52fe-4d7c-b9da-129b233962c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al crear clientes de GCP: La variable de entorno 'gcp_service_account' no está definida o está vacía.\n",
            "Error inesperado: 'NoneType' object has no attribute 'query'\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q3_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    288.1 MiB    288.1 MiB           1   @profile\n",
            "    10                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza análisis de tiempo en un archivo Parquet para obtener las menciones más comunes.\n",
            "    13                                         \n",
            "    14                                             Parameters:\n",
            "    15                                                 file_path (str): tabla donde se hara la consulta.\n",
            "    16                                         \n",
            "    17                                             Returns:\n",
            "    18                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene las menciones más comunes\n",
            "    19                                                 y su frecuencia.\n",
            "    20                                             \"\"\"\n",
            "    21    288.1 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    22                                         \n",
            "    23    288.1 MiB      0.0 MiB           1       try:\n",
            "    24                                                 # Crear clientes de GCP para BigQuery\n",
            "    25    288.1 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    26                                         \n",
            "    27                                                 # Query para contar nombres de usuario en tweets\n",
            "    28    288.1 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    29                                                     SELECT \n",
            "    30                                                       username,\n",
            "    31                                                       COUNT(*) AS count\n",
            "    32                                                     FROM (\n",
            "    33                                                       SELECT \n",
            "    34                                                         ARRAY_TO_STRING(REGEXP_EXTRACT_ALL(mentionedUsers, r\"'username':\\s*'([^']+)'\"), ',') AS usernames\n",
            "    35                                                       FROM \n",
            "    36    288.1 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    37                                                       WHERE mentionedUsers != \"None\"\n",
            "    38                                                     ) AS usernames_table\n",
            "    39                                                     CROSS JOIN UNNEST(SPLIT(usernames, ',')) AS username\n",
            "    40                                                     GROUP BY \n",
            "    41                                                       username\n",
            "    42                                                     ORDER BY \n",
            "    43                                                       count DESC\n",
            "    44                                                     LIMIT \n",
            "    45                                                       10;\n",
            "    46                                                 \"\"\"\n",
            "    47                                         \n",
            "    48                                                 # Ejecutar la consulta\n",
            "    49    288.1 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    50                                         \n",
            "    51                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    52                                                 results = query_job.result()\n",
            "    53                                                 end_time = time.time()\n",
            "    54                                                 execution_time = end_time - start_time\n",
            "    55                                                 print(\"Execution Time: \", execution_time)\n",
            "    56                                                 return [(row.username, row.count) for row in results]\n",
            "    57    288.1 MiB      0.0 MiB           1       except Exception as e:\n",
            "    58    288.1 MiB      0.0 MiB           1           print(f\"Error inesperado: {e}\")\n",
            "    59    288.1 MiB      0.0 MiB           1           return []\n",
            "\n",
            "\n",
            "Time Optimization - Results: []\n"
          ]
        }
      ],
      "source": [
        "results = q3_time(TABLE_NAME)\n",
        "print(f\"Time Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iXbZgJAabmu"
      },
      "source": [
        "## Consultas de Memoria en BigQuery: Función `q3_memory`\n",
        "\n",
        "\n",
        "### Ventajas del Enfoque\n",
        "\n",
        "Para el presente caso se considera las mismas ventajas, mencionadas anteriormente. Se espera discutir sobre la manera de la ejecución SQL sobre Bigquery.\n",
        "###  Descripción General\n",
        "\n",
        "Obtención del top 10 de usuarios mencionados, buscando optimizar el tiempo. Mediante el uso de Bigquery sobre python.\n",
        "\n",
        "### Descripción de Componentes\n",
        "- Importación de Librerías\n",
        "- Función q3_memory\n",
        "- Creación de Clientes de GCP\n",
        "- Construcción y Ejecución de la Consulta\n",
        "- Procesamiento de Resultados\n",
        "- Medición del Tiempo de Ejecución\n",
        "\n",
        "A continuación se muestran los resultados al ejecutar el proceso dentro de Github Actions\n",
        "![imagen.png](../img/memoria3a.png)\n",
        "![imagen.png](../img/memoria3b.png)\n",
        "\n",
        "Siendo un tiempo de 0,66 segundos y 249,3 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYA_mIq1GkQD",
        "outputId": "a92a28b9-4c8e-4893-97eb-54a24f502c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al crear clientes de GCP: La variable de entorno 'gcp_service_account' no está definida o está vacía.\n",
            "Error inesperado: 'NoneType' object has no attribute 'query'\n",
            "Filename: /content/drive/MyDrive/repos/latam_challenge_data_eng/src/q3_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     9    288.1 MiB    288.1 MiB           1   @profile\n",
            "    10                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    11                                             \"\"\"\n",
            "    12                                             Realiza consultas de memoria en BigQuery para obtener los \n",
            "    13                                             10 nombres de usuario más mencionados en tweets.\n",
            "    14                                         \n",
            "    15                                             Parameters:\n",
            "    16                                                 file_path (str): La ruta del archivo en BigQuery.\n",
            "    17                                         \n",
            "    18                                             Returns:\n",
            "    19                                                 List[Tuple[str, int]]: Una lista de tuplas que contiene \n",
            "    20                                                 los 10 nombres de usuario más mencionados\n",
            "    21                                                 y su frecuencia en los tweets.\n",
            "    22                                             \"\"\"\n",
            "    23    288.1 MiB      0.0 MiB           1       start_time = time.time()\n",
            "    24                                         \n",
            "    25    288.1 MiB      0.0 MiB           1       try:\n",
            "    26                                                 # Crear clientes de GCP para BigQuery\n",
            "    27    288.1 MiB      0.0 MiB           1           _, bigquery_client = create_gcp_clients()\n",
            "    28                                         \n",
            "    29                                                 # Query para contar nombres de usuario en tweets\n",
            "    30    288.1 MiB      0.0 MiB           2           query = fr\"\"\"\n",
            "    31                                                     -- Subconsulta para extraer los nombres de usuario y contarlos\n",
            "    32                                                     WITH usernames_table AS (\n",
            "    33                                                         SELECT \n",
            "    34                                                             REGEXP_EXTRACT_ALL(mentionedUsers, r\"'username':\\s*'([^']+)'\") AS usernames\n",
            "    35                                                         FROM \n",
            "    36    288.1 MiB      0.0 MiB           1                       `{file_path}`\n",
            "    37                                                     )\n",
            "    38                                         \n",
            "    39                                                     -- Consulta principal para contar los nombres de usuario y ordenarlos\n",
            "    40                                                     SELECT \n",
            "    41                                                         username,\n",
            "    42                                                         COUNT(*) AS count\n",
            "    43                                                     FROM \n",
            "    44                                                     (\n",
            "    45                                                         SELECT username FROM usernames_table, UNNEST(usernames) AS username\n",
            "    46                                                     ) AS user_data\n",
            "    47                                                     GROUP BY \n",
            "    48                                                         username\n",
            "    49                                                     ORDER BY \n",
            "    50                                                         count DESC\n",
            "    51                                                     LIMIT \n",
            "    52                                                         10;\n",
            "    53                                                 \"\"\"\n",
            "    54                                         \n",
            "    55                                                 # Ejecutar la consulta\n",
            "    56    288.1 MiB      0.0 MiB           1           query_job = bigquery_client.query(query)\n",
            "    57                                         \n",
            "    58                                                 # Obtener los resultados y devolverlos como una lista de tuplas\n",
            "    59                                                 results = query_job.result()\n",
            "    60                                                 end_time = time.time()\n",
            "    61                                                 execution_time = end_time - start_time\n",
            "    62                                                 print(\"Execution Time: \", execution_time)\n",
            "    63                                                 return [(row.username, row.count) for row in results]\n",
            "    64    288.1 MiB      0.0 MiB           1       except Exception as e:\n",
            "    65    288.1 MiB      0.0 MiB           1           print(f\"Error inesperado: {e}\")\n",
            "    66    288.1 MiB      0.0 MiB           1           return []\n",
            "\n",
            "\n",
            "Time Optimization - Results: []\n"
          ]
        }
      ],
      "source": [
        "results = q3_memory(TABLE_NAME)\n",
        "print(f\"Memory Optimization - Results: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M-lNyO7e1rk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
